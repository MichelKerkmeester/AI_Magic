# Dispatch Decision - When to Dispatch vs Direct Handling

Dispatch only when parallel execution or specialization benefits exceed orchestration overhead - measure benefit in time saved, not tasks completed.

> **Implementation Note**: The `orchestrate-skill-validation.sh` hook operates in **informational mode** (always exits 0). It calculates complexity scores and writes recommendations to `.claude/hooks/logs/orchestrator.log`, but the AI agent makes the final dispatch decision.

**Prerequisites:** Follow [Agent Orchestrator Workflow](../SKILL.md) for complete context:
- **Complexity Scoring**: See [complexity_scoring.md](./complexity_scoring.md) for threshold calculation
- **Skill Clustering**: See [skill_clustering.md](./skill_clustering.md) for agent creation

---

## 1. üå≤ THE DECISION TREE

You MUST evaluate ALL checkpoints in order before dispatching.

```
START
  ‚îÇ
  ‚îú‚îÄ [Checkpoint 1: Token Budget]
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ Budget < 20% ‚Üí DIRECT (insufficient resources)
  ‚îÇ   ‚îî‚îÄ Budget ‚â• 20% ‚Üí Continue
  ‚îÇ
  ‚îú‚îÄ [Checkpoint 2: Complexity Score]
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ Score < 40% ‚Üí DIRECT (too simple)
  ‚îÇ   ‚îú‚îÄ Score 40-49% ‚Üí COLLABORATIVE (ask user)
  ‚îÇ   ‚îî‚îÄ Score ‚â• 50% ‚Üí Continue
  ‚îÇ
  ‚îú‚îÄ [Checkpoint 3: Domain Count]
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ Domains = 1 ‚Üí DIRECT (no specialization benefit)
  ‚îÇ   ‚îî‚îÄ Domains ‚â• 2 ‚Üí Continue
  ‚îÇ
  ‚îú‚îÄ [Checkpoint 4: Dependencies]
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ Sequential only ‚Üí DIRECT (no parallel benefit)
  ‚îÇ   ‚îî‚îÄ Parallel possible ‚Üí Continue
  ‚îÇ
  ‚îú‚îÄ [Checkpoint 5: Overhead Analysis]
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ Overhead > 30% ‚Üí DIRECT (overhead too high)
  ‚îÇ   ‚îî‚îÄ Overhead < 30% ‚Üí DISPATCH
  ‚îÇ
  ‚îî‚îÄ DISPATCH SUB-AGENTS
```

**Validation**: `dispatch_decision_made`

---

## 2. ‚úÖ DISPATCH BENEFICIAL SCENARIOS

### Scenario 1: Multi-Domain Tasks
**Pattern**: Task spans 3+ distinct functional areas

‚ùå **WRONG - Single domain misidentified**:
```
Request: "Fix bug and add comment explaining the fix"
Analysis: 2 domains (code + documentation)
Decision: DISPATCH
‚Üí Incorrect: Comment is trivial, not separate domain
```

‚úÖ **RIGHT - True multi-domain**:
```
Request: "Refactor auth, update API docs, add tests, prepare PR"
Analysis: 4 domains (code, documentation, testing, git)
Decision: DISPATCH
Benefit: Parallel execution of independent domains
Expected speedup: 50-60% faster than sequential
```

**Validation**: `multi_domain_identified`


### Scenario 2: Parallel Debugging
**Pattern**: Multiple independent failures in unrelated modules

‚ùå **WRONG - Missing sequential dependency**:
```
Request: "Fix bug that breaks tests, then fix the tests"
Analysis: 2 bugs, parallel opportunity
Decision: DISPATCH
‚Üí Incorrect: Tests depend on bug fix, must be sequential
```

‚úÖ **RIGHT - True parallel opportunity**:
```
Request: "Fix failing tests in auth, payment, and shipping"
Analysis: 3 independent test suites, no dependencies
Decision: DISPATCH
Benefit: 3x faster via parallel investigation
Expected agents: 3 (one per module)
```

**Validation**: `parallel_opportunity_confirmed`


### Scenario 3: Complex Feature Implementation
**Pattern**: Large feature with separable concerns

‚úÖ **RIGHT - Separable concerns**:
```
Request: "Implement dark mode with theme switcher, preferences, docs"
Components: UI logic, state management, persistence, documentation
Decision: DISPATCH
Benefit: Specialized agents for each component
Expected agents: 3-4
```

**Validation**: `complex_feature_identified`


### Scenario 4: Broad Codebase Analysis
**Pattern**: System-wide search and documentation

‚úÖ **RIGHT - Wide-ranging analysis**:
```
Request: "Find all API endpoints and document their usage"
Scope: Full codebase search + documentation generation
Decision: DISPATCH
Benefit: Parallel search and documentation creation
Expected agents: 2 (search + docs)
```

**Validation**: `broad_analysis_identified`

---

## 3. ‚ùå DIRECT HANDLING SCENARIOS

### Scenario 1: Single File Changes
**Pattern**: Localized modifications

‚úÖ **RIGHT - Direct for simple changes**:
```
Request: "Fix typo in README"
Scope: 1 file, 1 line
Complexity: 5%
Overhead: 2-3 seconds (>100% of task time)
Decision: DIRECT
Rationale: Overhead exceeds task duration
```

**Validation**: `single_file_identified`


### Scenario 2: Sequential Dependencies
**Pattern**: Step B requires step A completion

‚ùå **WRONG - Ignoring dependencies**:
```
Request: "Install dependencies, build project, run tests"
Decision: DISPATCH (3 agents in parallel)
‚Üí Incorrect: Each step depends on previous completion
```

‚úÖ **RIGHT - Recognizing sequence**:
```
Request: "Install dependencies, build project, run tests"
Dependencies: build needs install, tests need build
Decision: DIRECT
Rationale: No parallelization possible, overhead not justified
```

**Validation**: `sequential_dependency_identified`


### Scenario 3: Quick Fixes
**Pattern**: Trivial changes < 5 minutes

‚úÖ **RIGHT - Direct for quick tasks**:
```
Request: "Add missing semicolon in auth.js"
Time estimate: 30 seconds
Overhead: 2-3 seconds dispatch
Decision: DIRECT
Rationale: Task too trivial for orchestration
```

**Validation**: `quick_fix_identified`


### Scenario 4: Exploratory Tasks
**Pattern**: Unclear scope requiring iteration

‚úÖ **RIGHT - Direct for exploration**:
```
Request: "Figure out why this is broken"
Scope: Unknown, requires investigation
Approach: Iterative discovery
Decision: DIRECT
Rationale: Cannot pre-plan agents for unknown problem
```

**Validation**: `exploratory_task_identified`

---

## 4. ü§ù COLLABORATIVE DECISION (MEDIUM COMPLEXITY)

### When to Ask User

Complexity score 40-49% triggers user preference:

```markdown
Complexity Score: 45%

This task has medium complexity (45%). I can either:

A) **Handle directly** (simpler, sequential execution)
   - Pros: Less overhead, easier to track
   - Cons: May take longer, sequential processing
   - Estimated time: 15 minutes

B) **Create specialized sub-agents** (parallel execution)
   - Pros: Potentially faster (est. 10 min), specialized focus
   - Cons: More complex, 2-3s overhead per agent
   - Estimated agents: 2-3

Which would you prefer?
```

**User Override Patterns**:
- User may prefer dispatch for learning/testing
- User may prefer direct for simplicity
- Track preferences to learn user patterns

**Validation**: `user_preference_collected`

---

## 5. üìä OVERHEAD CALCULATION

### Fixed Overhead Per Agent

```
Sub-agent creation: ~500ms
Task tool invocation: ~1s
Result integration: ~500ms
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total fixed: ~2s per agent
```

### Variable Overhead

```
Context preparation: 100-500ms
Skill filtering: 50-100ms
Domain clustering: 100-200ms
Error handling: 0-1000ms (if needed)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total variable: 250-1800ms
```

### Break-Even Analysis

**Decision Logic**:
```markdown
parallel_time = estimated_task_time / num_agents
overhead = num_agents * 2.5s
total_time = parallel_time + overhead

IF total_time < (sequential_time * 0.7):
  ‚Üí DISPATCH (30%+ time savings justifies overhead)
ELSE:
  ‚Üí DIRECT (overhead not justified)
```

‚ùå **WRONG - Ignoring overhead**:
```
Task: 5-minute fix, 3 agents
Overhead ignored
Decision: DISPATCH
‚Üí Actual: 2min parallel + 7.5s overhead vs 5min direct
```

‚úÖ **RIGHT - Including overhead**:
```
Task: 15-minute implementation, 3 agents
parallel_time = 15min / 3 = 5min
overhead = 3 * 2.5s = 7.5s
total = 5min 7.5s vs 15min sequential
savings = 66% ‚Üí DISPATCH justified
```

**Validation**: `overhead_calculated`

---

## 6. ‚öôÔ∏è SPECIAL CONSIDERATIONS

### Token Budget Constraints

**Decision Logic**:
```markdown
IF token_budget < 20%:
  ‚Üí Override: DIRECT (force direct regardless of complexity)

IF token_budget >= 20% AND token_budget < 40%:
  ‚Üí Override: LIMIT_2 (maximum 2 sub-agents only)

IF token_budget >= 40%:
  ‚Üí Normal operation (up to 5 agents allowed)
```

**Validation**: `token_budget_checked`


### Error Recovery Cost

Consider failure probability:

```markdown
IF task_failure_probability > 30%:
  ‚Üí Prefer DIRECT (easier debugging and recovery)

IF task_failure_probability < 10%:
  ‚Üí DISPATCH acceptable (low risk)
```

**Validation**: `failure_risk_assessed`


### User Preference Learning

Track and learn from user choices:

```
[timestamp] USER_PREFERENCE
Task type: refactoring
Complexity: 55%
Offered: COLLABORATIVE
User chose: DISPATCH
Note: User prefers parallel for refactoring tasks
```

**After 10+ decisions**: Adjust default behavior based on patterns

**Validation**: `user_patterns_logged`

---

## 7. üêõ COMMON DECISION ERRORS

**Error Pattern 1: Over-Dispatching Simple Tasks**
```javascript
// Request: "Update variable name"
// ‚ùå WRONG: Created agent (overhead > task time)
// ‚úÖ RIGHT: Handle directly (30-second task)
```

**Error Pattern 2: Missing Sequential Dependencies**
```javascript
// Request: "Build then test"
// ‚ùå WRONG: Parallel agents (tests need build)
// ‚úÖ RIGHT: Sequential handling (dependency chain)
```

**Error Pattern 3: Ignoring Token Budget**
```javascript
// Complexity: 80%, Token Budget: 15%
// ‚ùå WRONG: Dispatched anyway (failed mid-execution)
// ‚úÖ RIGHT: Override to DIRECT (insufficient budget)
```

**Error Pattern 4: Underestimating Overhead**
```javascript
// Task: 3 minutes, 4 agents
// ‚ùå WRONG: Ignored 10s overhead (assumed negligible)
// ‚úÖ RIGHT: 45s parallel + 10s overhead = still beneficial
```

---

## 8. üìà TUNING PARAMETERS

### Adjustable Thresholds

```javascript
const THRESHOLDS = {
  complexity: {
    direct: 40,          // Below: always direct
    collaborative: 50,   // Below: ask user
    dispatch: 50         // At/above: auto-dispatch
  },
  overhead: {
    max_percent: 30      // Max acceptable overhead
  },
  domains: {
    min_for_dispatch: 2  // Minimum domains for dispatch
  },
  tokens: {
    critical: 20,        // Force direct below this
    limited: 40          // Limit agents below this
  }
};
```

### Metrics for Tuning

Track actual outcomes:

```markdown
[timestamp] DECISION_OUTCOME
Decision: DISPATCH
Predicted benefit: 40% faster
Actual benefit: 35% faster
Accuracy: 87.5%
Adjustment needed: NO (within 10% tolerance)
```

**Iteration Process**:
1. Track 20+ decisions
2. Calculate accuracy rate (predicted vs actual)
3. Adjust thresholds by ¬±5% if accuracy <80%
4. Repeat until 80%+ accuracy sustained

---

## 9. üéØ QUICK REFERENCE MATRIX

| Complexity | Domains | Parallel | Token Budget | Decision |
|------------|---------|----------|--------------|----------|
| <40% | Any | Any | Any | DIRECT |
| Any | 1 | Any | Any | DIRECT |
| Any | Any | None | Any | DIRECT |
| Any | Any | Any | <20% | DIRECT |
| 40-49% | 2+ | Some | >20% | COLLABORATIVE |
| ‚â•50% | 2+ | Yes | >40% | DISPATCH |
| ‚â•50% | 2+ | Yes | 20-40% | LIMIT_DISPATCH (max 2) |

---

**See also:**
- [complexity_scoring.md](./complexity_scoring.md) for threshold calculation
- [skill_clustering.md](./skill_clustering.md) for agent creation after dispatch decision
- [sub_agent_lifecycle.md](./sub_agent_lifecycle.md) for execution details