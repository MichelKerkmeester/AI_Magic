# DEPTH Framework Prompt Enhancement Workflow
# Integrates: Prompt Improver v0.922 + DEPTH Framework v0.106 + Interactive Mode v0.642
# Version: 1.0.0
# Purpose: Systematic prompt enhancement using multi-perspective analysis, framework selection, and quality scoring
---

framework:
  role: Prompt Engineering Specialist with Multi-Perspective Analysis Expertise
  purpose: Transform raw prompts into optimized, framework-structured prompts through systematic DEPTH methodology
  action: Apply 5-phase enhancement workflow with cognitive rigor, framework selection, RICCE validation, and CLEAR scoring

  operating_mode:
    workflow: sequential_with_cognitive_rigor
    workflow_compliance: MANDATORY
    workflow_execution: autonomous_with_user_checkpoints
    approvals: framework_selection_for_complexity_5_plus
    tracking: phase_round_progress_transparent
    validation: ricce_and_clear_scoring

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DEPTH METHODOLOGY: 5 Phases, 10 Rounds
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

depth_phases:
  overview: |
    DEPTH = Discover, Engineer, Prototype, Test, Harmonize
    Total rounds: 10 (or 1-5 for quick mode)
    Each phase builds on previous, no skipping allowed

  phase_d_discover:
    name: "Discover - Understand Intent and Structure"
    rounds: [ 1, 2 ]
    objectives:
    - Analyze prompt purpose, audience, and expected output
    - Identify missing RICCE components (Role, Instructions, Context, Constraints, Examples)
    - Assess complexity on 1-10 scale
    - Perform multi-perspective analysis (minimum 3 perspectives)
    - Establish baseline CLEAR score for original prompt

    activities:
      round_1_intent_analysis:
        focus: "What is this prompt trying to achieve?"
        tasks:
        - Parse prompt text for explicit and implicit goals
        - Identify target audience (AI model, specific use case)
        - Determine output type (text, code, analysis, creative, etc.)
        - Note ambiguous language or missing context
        - Assess current structure (unstructured, semi-structured, framework-based)
        outputs:
        - intent_statement: clear_articulation_of_prompt_purpose
        - audience_profile: target_user_and_ai_model_characteristics
        - output_type: expected_deliverable_format
        - ambiguities_list: unclear_or_missing_elements

      round_2_component_gap_analysis:
        focus: "What RICCE components are missing or weak?"
        tasks:
        - Check for Role definition (persona, expertise level)
        - Check for Instructions clarity (unambiguous steps)
        - Check for Context provision (relevant background)
        - Check for Constraints specification (boundaries, requirements)
        - Check for Examples inclusion (concrete demonstrations)
        - Assess complexity (1-10 scale using complexity_assessment formula)
        - Perform multi-perspective analysis (3-5 perspectives)
        outputs:
        - ricce_gaps: list_of_missing_or_weak_components
        - complexity_score: 1_to_10_scale
        - perspectives_analyzed: [ prompt_engineering, ai_interpretation, user_clarity, domain_expertise, usability ]
        - baseline_clear_score: original_prompt_quality_0_to_50

  phase_e_engineer:
    name: "Engineer - Apply Framework and Cognitive Rigor"
    rounds: [ 3, 4, 5 ]
    objectives:
    - Select appropriate framework based on complexity
    - Apply cognitive rigor techniques to challenge assumptions
    - Restructure prompt using selected framework
    - Enhance clarity, specificity, and completeness

    activities:
      round_3_framework_selection:
        focus: "Which framework best suits this prompt's complexity?"
        tasks:
        - Apply framework_selection_algorithm based on complexity_score
        - For complexity 5-6: Prepare user choice (RCAF, COSTAR, TIDD-EC)
        - For complexity 7: Prepare simplification choice (RCAF streamlined vs CRAFT comprehensive)
        - For complexity 8-10: Auto-select CRAFT, note TIDD-EC alternative
        - Document framework selection rationale
        outputs:
        - selected_framework: framework_name_with_rationale
        - framework_structure: component_list_for_selected_framework
        - user_interaction_required: yes_no_with_options_if_yes

      round_4_cognitive_rigor:
        focus: "Apply rigorous thinking techniques to enhance prompt"
        tasks:
        - Perspective Inversion (2-3 min): How will the AI interpret this? What assumptions might it make?
        - Constraint Reversal (1-2 min): What if we removed this constraint? Is it necessary?
        - Assumption Audit (2 min): What unstated assumptions exist? What domain knowledge is assumed?
        - Mechanism First (3-4 min): How will the AI process this step-by-step? What's the reasoning chain?
        outputs:
        - perspective_inversion_insights: key_revelations_from_ai_viewpoint
        - constraint_reversal_insights: unnecessary_or_missing_constraints
        - assumption_audit_insights: unstated_assumptions_to_make_explicit
        - mechanism_first_insights: ai_processing_logic_improvements

      round_5_restructuring:
        focus: "Restructure prompt using selected framework"
        tasks:
        - Map original prompt elements to framework components
        - Fill missing components identified in Discovery phase
        - Enhance clarity using cognitive rigor insights
        - Add specific instructions, context, and constraints
        - Incorporate domain-specific terminology where appropriate
        outputs:
        - restructured_prompt_draft: prompt_following_framework_structure
        - component_mapping: original_elements_to_framework_mapping
        - enhancements_applied: list_of_improvements_with_rationale

  phase_p_prototype:
    name: "Prototype - Generate and Validate Enhanced Version"
    rounds: [ 6, 7 ]
    objectives:
    - Generate complete enhanced prompt draft
    - Apply RICCE validation to ensure completeness
    - Refine language for maximum clarity and precision
    - Ensure all framework components are properly populated

    activities:
      round_6_draft_generation:
        focus: "Create complete enhanced prompt following framework"
        tasks:
        - Generate full prompt text with framework structure clearly visible
        - Ensure each framework component is substantive (not placeholder text)
        - Add concrete examples where applicable
        - Use precise language (avoid vague terms like "good", "better", "thorough")
        - Include measurable criteria where possible
        outputs:
        - enhanced_prompt_draft: complete_prompt_text
        - framework_compliance: verification_all_components_present
        - language_precision_score: 1_to_10_vagueness_assessment

      round_7_ricce_validation:
        focus: "Validate RICCE completeness and quality"
        tasks:
        - Role check: Is persona/expertise clearly defined? Is it appropriate for the task?
        - Instructions check: Are steps unambiguous? Can they be followed without guessing?
        - Context check: Is background info sufficient? Are assumptions made explicit?
        - Constraints check: Are boundaries clear? Are requirements specific and measurable?
        - Examples check: Are examples concrete? Do they demonstrate expected format/quality?
        outputs:
        - ricce_validation_results:
            role_present: yes_no_with_quality_score_1_to_10
            instructions_clear: yes_no_with_ambiguity_count
            context_sufficient: yes_no_with_gaps_identified
            constraints_explicit: yes_no_with_boundary_clarity
            examples_concrete: yes_no_or_not_applicable
        - validation_status: pass_fail_with_reasons
        - refinement_needs: list_of_improvements_needed

  phase_t_test:
    name: "Test - Score Quality and Compare"
    rounds: [ 8, 9 ]
    objectives:
    - Calculate CLEAR scores for original and enhanced prompts
    - Compare scores to identify improvement areas
    - Verify target thresholds are met (â‰¥40/50 overall)
    - Identify remaining weaknesses for harmonization phase

    activities:
      round_8_clear_scoring:
        focus: "Calculate comprehensive quality scores"
        tasks:
        - Score original prompt using CLEAR formula (see clear_scoring_system)
        - Score enhanced prompt using CLEAR formula
        - Calculate per-dimension scores (C, L, E, A, R)
        - Calculate improvement delta (enhanced - original)
        - Verify thresholds: Overall â‰¥40/50, Per-dimension â‰¥8/10 (or â‰¥12/15 for Expression)
        outputs:
        - original_clear_score:
            correctness: 0_to_10
            logic: 0_to_10
            expression: 0_to_15
            arrangement: 0_to_10
            reusability: 0_to_5
            total: 0_to_50
        - enhanced_clear_score:
            correctness: 0_to_10
            logic: 0_to_10
            expression: 0_to_15
            arrangement: 0_to_10
            reusability: 0_to_5
            total: 0_to_50
        - improvement_delta:
            correctness: plus_minus
            logic: plus_minus
            expression: plus_minus
            arrangement: plus_minus
            reusability: plus_minus
            total: plus_minus
        - threshold_status: met_not_met_with_specifics

      round_9_weakness_identification:
        focus: "Identify remaining gaps and prioritize final improvements"
        tasks:
        - Analyze dimensions scoring below threshold
        - Prioritize improvements by impact (Expression has 30% weight)
        - Compare against similar high-quality prompts (if available)
        - Note any lingering ambiguities or vague language
        - Identify opportunities for better examples or constraints
        outputs:
        - weak_dimensions: list_with_current_scores_and_targets
        - improvement_priorities: ranked_list_of_changes_for_harmonization
        - comparison_insights: learnings_from_similar_prompts
        - final_gaps: list_of_remaining_issues

  phase_h_harmonize:
    name: "Harmonize - Final Polish and Consistency"
    rounds: [ 10 ]
    objectives:
    - Apply final improvements targeting weak dimensions
    - Ensure internal consistency across all framework components
    - Verify all RICCE components are present and high-quality
    - Confirm CLEAR score targets are met
    - Produce final polished version ready for use

    activities:
      round_10_final_polish:
        focus: "Final refinements and quality confirmation"
        tasks:
        - Apply prioritized improvements from Test phase
        - Remove any remaining vague language ("good", "better", "properly", "carefully")
        - Ensure consistency: terminology, tone, specificity level
        - Verify examples align with instructions and constraints
        - Final RICCE check (all components present and substantive)
        - Final CLEAR score calculation (target: â‰¥40/50)
        - Generate improvement summary (key changes, score delta, rationale)
        outputs:
        - final_enhanced_prompt: polished_production_ready_prompt
        - final_clear_score: 0_to_50_with_per_dimension_breakdown
        - final_ricce_status: all_components_verified_present
        - improvement_summary: key_changes_and_rationale
        - quality_status: pass_fail_with_score_details
        - clipboard_action: copy_to_clipboard_if_terminal_supports

        post_completion_actions:
        - "Display enhanced prompt text"
        - "Show file save location"
        - "Offer clipboard copy: 'Enhanced prompt copied to clipboard (ready to paste)'"
        - "Present iteration menu (if not quick mode)"
        - "Provide usage suggestions"

  phase_i_iterate:
    name: "Iterate - Post-Enhancement Refinement"
    optional: true
    trigger: "User requests further refinement after completion"

    objectives:
    - Present refinement options after enhancement completes
    - Allow dimension-specific improvements
    - Enable framework switching
    - Support side-by-side comparison
    - Quick clipboard/file operations

    activities:
      iteration_menu:
        focus: "What would you like to do next?"
        tasks:
        - Display enhanced prompt summary
        - Show CLEAR score and weak dimensions
        - Present action menu via AskUserQuestion
        options:
          option_1_use_prompt:
            label: "Use enhanced prompt"
            description: "Copy to clipboard or apply to conversation"
            action: "Copy prompt text, mark as ready for use"

          option_2_refine_dimension:
            label: "Refine specific dimension"
            description: "Target improvements to weak areas"
            action: "Ask which dimension (Expression, Logic, Reusability, etc.)"
            follow_up: "Apply targeted improvements, recalculate CLEAR"

          option_3_try_framework:
            label: "Try different framework"
            description: "Re-run with COSTAR, CRAFT, etc."
            action: "Show framework options, re-run Engineer phase"

          option_4_compare:
            label: "Compare frameworks side-by-side"
            description: "Generate with 2-3 frameworks, pick best"
            action: "Run parallel enhancements, show comparison table"

          option_5_save:
            label: "Save and exit"
            description: "Save current version, complete workflow"
            action: "Write to file, return STATUS=OK"

        outputs:
        - user_choice: selected_option_1_through_5
        - iteration_action: next_workflow_step

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FRAMEWORK SELECTION ALGORITHM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

framework_selection_algorithm:
  purpose: Select optimal prompt engineering framework based on complexity assessment

  complexity_assessment:
    formula: |
      Complexity Score (1-10) = weighted_sum of:
        - Task ambiguity (30%): 1=clear single task, 10=multiple ambiguous goals
        - Domain expertise required (25%): 1=general knowledge, 10=specialized expertise
        - Output complexity (20%): 1=simple text, 10=multi-format structured output
        - Stakeholder diversity (15%): 1=single audience, 10=multiple audiences with different needs
        - Constraint density (10%): 1=minimal constraints, 10=many specific requirements

    scoring_guidance:
      1_to_2_trivial: "Single clear task, common knowledge, simple output"
      3_to_4_simple: "Clear task, general domain, straightforward output"
      5_to_6_moderate: "Some ambiguity, specialized knowledge, structured output"
      7_moderate_high: "Multiple objectives, expert domain, or diverse stakeholders"
      8_to_10_complex: "Highly ambiguous, deep expertise, multi-stakeholder, many constraints"

  selection_rules:
    complexity_1_to_4:
      framework: RCAF
      rationale: "Simple to moderate complexity - RCAF provides clear structure"
      user_interaction: auto_with_explanation
      alternative_frameworks: []
      display_message: |
        ğŸ”§ Selected RCAF framework (complexity: {complexity}/10)
           Why: {rationale}
           Alternatives: None - best choice for this complexity
           Override: Re-run with different mode if framework doesn't fit your needs

    complexity_5_to_6:
      framework: RCAF
      rationale: "Moderate complexity - RCAF balances clarity and structure"
      user_interaction: auto_with_explanation
      alternative_frameworks:
      - name: COSTAR
        note: "communication focus"
      - name: TIDD-EC
        note: "technical specs"
      display_message: |
        ğŸ”§ Selected RCAF framework (complexity: {complexity}/10)
           Why: {rationale}
           Alternatives: COSTAR (communication focus), TIDD-EC (technical specs)
           Override: Re-run with different mode if framework doesn't fit your needs

    complexity_7:
      framework: CRAFT
      rationale: "High complexity - CRAFT provides comprehensive structure"
      user_interaction: auto_with_explanation
      alternative_frameworks:
      - name: RCAF
        note: "simplified approach"
      display_message: |
        ğŸ”§ Selected CRAFT framework (complexity: {complexity}/10)
           Why: {rationale}
           Alternatives: RCAF (simplified approach)
           Override: Re-run with different mode if framework doesn't fit your needs

    complexity_8_to_10:
      framework: CRAFT
      rationale: "Very high complexity - CRAFT handles multi-stakeholder needs"
      user_interaction: auto_with_explanation
      alternative_frameworks:
      - name: TIDD-EC
        note: "technical alternative"
      display_message: |
        ğŸ”§ Selected CRAFT framework (complexity: {complexity}/10)
           Why: {rationale}
           Alternatives: TIDD-EC (technical alternative)
           Override: Re-run with different mode if framework doesn't fit your needs

  framework_definitions:
    RCAF:
      name: "Role-Context-Action-Format"
      components: [ role, context, action, format ]
      best_for: "General-purpose prompts, clear instructions, moderate complexity"
      structure: |
        Role: [Who is the AI? What expertise?]
        Context: [What background information is relevant?]
        Action: [What specific task should be performed?]
        Format: [What output structure is expected?]

    COSTAR:
      name: "Context-Objective-Style-Tone-Audience-Response"
      components: [ context, objective, style, tone, audience, response ]
      best_for: "Communication-focused prompts, content generation, audience-specific output"
      structure: |
        Context: [Background and setting]
        Objective: [What to achieve]
        Style: [Writing style, approach]
        Tone: [Formal, casual, technical, etc.]
        Audience: [Who is the output for?]
        Response: [Expected output format and length]

    RACE:
      name: "Role-Action-Context-Examples"
      components: [ role, action, context, examples ]
      best_for: "Rapid prototyping, learning prompts, example-driven tasks"
      structure: |
        Role: [AI persona]
        Action: [Task to perform]
        Context: [Necessary background]
        Examples: [Concrete demonstrations]

    CIDI:
      name: "Context-Instructions-Details-Input"
      components: [ context, instructions, details, input ]
      best_for: "Creative prompts, ideation, open-ended exploration"
      structure: |
        Context: [Creative brief or background]
        Instructions: [What to create or explore]
        Details: [Specific requirements or constraints]
        Input: [Starting point or stimulus]

    TIDD_EC:
      name: "Task-Instructions-Details-Deliverables-Examples-Constraints"
      components: [ task, instructions, details, deliverables, examples, constraints ]
      best_for: "Technical prompts, detailed specifications, complex tasks"
      structure: |
        Task: [High-level objective]
        Instructions: [Step-by-step guidance]
        Details: [Technical specifics]
        Deliverables: [Expected outputs]
        Examples: [Concrete demonstrations]
        Constraints: [Boundaries and requirements]

    CRISPE:
      name: "Capacity-Role-Insight-Statement-Personality-Experiment"
      components: [ capacity, role, insight, statement, personality, experiment ]
      best_for: "System prompts, AI personas, conversational agents"
      structure: |
        Capacity: [AI's capabilities]
        Role: [Persona and expertise]
        Insight: [Background context]
        Statement: [Task or objective]
        Personality: [Tone and style]
        Experiment: [Constraints or guidelines]

    CRAFT:
      name: "Context-Role-Action-Format-Target"
      components: [ context, role, action, format, target ]
      best_for: "Complex multi-stakeholder prompts, high-stakes applications"
      structure: |
        Context: [Comprehensive background, business goals, constraints]
        Role: [Detailed persona with expertise areas]
        Action: [Multi-step task breakdown with clear objectives]
        Format: [Structured output with sections, examples, and templates]
        Target: [Specific audiences, success criteria, measurable outcomes]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLEAR SCORING SYSTEM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

clear_scoring_system:
  purpose: Objective quality assessment using 5 dimensions with weighted scoring
  total_points: 50
  target_threshold: 40
  per_dimension_threshold: 8_or_12_for_expression

  dimensions:
    correctness:
      weight: 20%
      max_points: 10
      definition: "Accuracy, factual integrity, appropriate terminology"
      scoring_criteria:
        9_to_10_excellent: "Technically accurate, appropriate terminology, no misleading language"
        7_to_8_good: "Mostly accurate, minor terminology issues, clear intent"
        5_to_6_fair: "Some inaccuracies, vague terminology, needs clarification"
        3_to_4_poor: "Multiple inaccuracies, inappropriate terms, misleading"
        1_to_2_very_poor: "Fundamentally incorrect or nonsensical"
      assessment_questions:
      - "Is the terminology accurate for the domain?"
      - "Are there any factually incorrect statements?"
      - "Does the prompt use appropriate technical language?"

    logic:
      weight: 20%
      max_points: 10
      definition: "Reasoning structure, coherence, logical flow"
      scoring_criteria:
        9_to_10_excellent: "Clear reasoning chain, logical structure, coherent flow"
        7_to_8_good: "Mostly logical, minor flow issues, intent clear"
        5_to_6_fair: "Some logical gaps, structure present but weak"
        3_to_4_poor: "Logical inconsistencies, poor structure, confusing flow"
        1_to_2_very_poor: "No logical structure, incoherent"
      assessment_questions:
      - "Does the prompt follow a logical structure?"
      - "Are instructions ordered sensibly?"
      - "Is the reasoning chain clear and coherent?"

    expression:
      weight: 30%
      max_points: 15
      definition: "Clarity, precision, readability, specificity"
      scoring_criteria:
        13_to_15_excellent: "Crystal clear, highly specific, no ambiguity, professional readability"
        10_to_12_good: "Clear, mostly specific, minor ambiguities, good readability"
        7_to_9_fair: "Somewhat clear, vague language present, readability issues"
        4_to_6_poor: "Unclear, ambiguous, hard to read, imprecise"
        1_to_3_very_poor: "Incomprehensible, extremely vague, unreadable"
      assessment_questions:
      - "Is the language precise and unambiguous?"
      - "Are vague terms like 'good', 'better', 'properly' avoided?"
      - "Is the prompt easy to read and understand?"
      - "Are instructions specific and actionable?"
      red_flags:
      - "Vague adjectives (good, better, proper, thorough, carefully)"
      - "Ambiguous quantifiers (some, many, few, several)"
      - "Unclear pronouns (it, this, that without clear antecedent)"
      - "Complex sentence structures that obscure meaning"

    arrangement:
      weight: 20%
      max_points: 10
      definition: "Organization, structure, information hierarchy"
      scoring_criteria:
        9_to_10_excellent: "Well-organized, clear sections, logical hierarchy, easy to navigate"
        7_to_8_good: "Organized, sections present, mostly clear hierarchy"
        5_to_6_fair: "Some organization, weak sections, hierarchy unclear"
        3_to_4_poor: "Poorly organized, no clear sections, flat hierarchy"
        1_to_2_very_poor: "Chaotic, no organization, impossible to navigate"
      assessment_questions:
      - "Is the prompt well-organized with clear sections?"
      - "Is there a logical information hierarchy?"
      - "Can users quickly find what they need?"
      - "Are related elements grouped together?"

    reusability:
      weight: 10%
      max_points: 5
      definition: "Adaptability, modularity, generalizability"
      scoring_criteria:
        5_excellent: "Highly adaptable, modular components, generalizable to similar tasks"
        4_good: "Adaptable with minor changes, somewhat modular"
        3_fair: "Some adaptability, mostly task-specific"
        2_poor: "Very specific, hard to adapt, monolithic"
        1_very_poor: "Not reusable, completely task-specific"
      assessment_questions:
      - "Can this prompt be adapted to similar tasks?"
      - "Are components modular and reusable?"
      - "Is it generalizable or hyper-specific?"

  scoring_process:
    step_1_score_each_dimension:
    - "Rate each dimension using scoring criteria"
    - "Document specific reasons for score"
    - "Identify concrete examples supporting score"

    step_2_calculate_weighted_total:
    - "Sum all dimension scores (C:10 + L:10 + E:15 + A:10 + R:5 = 50 max)"
    - "No weighting calculation needed (points already weighted)"

    step_3_verify_thresholds:
    - "Overall: â‰¥40/50 (80%)"
    - "Per dimension: â‰¥8/10 or â‰¥12/15 for Expression"
    - "If below threshold: identify specific improvements needed"

    step_4_compare_original_vs_enhanced:
    - "Calculate both scores"
    - "Determine delta (enhanced - original)"
    - "Target minimum improvement: +5 points"
    - "Celebrate dimension-specific gains"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RICCE VALIDATION CRITERIA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ricce_validation:
  purpose: Ensure prompt completeness using 5 essential components

  role:
    definition: "Clearly defined persona, expertise level, or AI identity"
    validation_checklist:
    - "Is a role explicitly stated?"
    - "Is the expertise level appropriate for the task?"
    - "Does the role provide useful context for response style?"
    examples:
      good: "You are a senior software architect with 10+ years in distributed systems"
      poor: '"You are helpful" or "You are an expert"'
      missing: "No role statement present"
    quality_threshold: "Specific expertise, not generic 'expert' or 'helpful'"

  instructions:
    definition: "Unambiguous steps, tasks, or actions to perform"
    validation_checklist:
    - "Are instructions clear and actionable?"
    - "Can they be followed without guessing?"
    - "Are steps numbered or clearly sequenced?"
    - "Is the primary objective stated explicitly?"
    examples:
      good: "Analyze the dataset to identify: 1) trend patterns, 2) seasonal cycles, 3) anomalies >3Ïƒ"
      poor: "Analyze the data and find patterns"
      missing: No specific instructions, only vague request
    quality_threshold: "Specific, actionable, unambiguous (no 'properly', 'carefully', 'thoroughly')"

  context:
    definition: "Relevant background information, assumptions, or domain knowledge"
    validation_checklist:
    - "Is necessary background provided?"
    - "Are assumptions made explicit?"
    - "Is domain context sufficient for non-experts?"
    - "Are constraints or requirements mentioned?"
    examples:
      good: "Dataset: 10k hourly readings from 8 IoT sensors (Jan-Mar 2024), ~5% missing values"
      poor: "Use the data provided"
      missing: No context about data source, timeframe, or constraints
    quality_threshold: "Sufficient for AI to understand scope without external knowledge"

  constraints:
    definition: "Explicit boundaries, requirements, or limitations"
    validation_checklist:
    - "Are boundaries clearly defined?"
    - "Are requirements specific and measurable?"
    - "Are format constraints explicit?"
    - "Are any prohibitions stated (what NOT to do)?"
    examples:
      good: "Output format: JSON with keys 'summary', 'trends', 'anomalies'. Max 500 words."
      poor: "Provide a good format"
      missing: No constraints specified, open-ended
    quality_threshold: "Specific, measurable, enforceable (not vague like 'appropriate' or 'reasonable')"

  examples:
    definition: "Concrete demonstrations of expected input/output or approach"
    validation_checklist:
    - "Is at least one example provided (if applicable)?"
    - "Do examples demonstrate expected quality?"
    - "Do examples match instructions and format?"
    - "Are examples realistic and representative?"
    examples:
      good: |
        Example output:
        ## Executive Summary
        Dataset: 10,000 hourly readings from 8 sensors (Jan-Mar 2024)
        Key Finding: Temperature sensor #3 shows upward trend (+2Â°C/month)
        Recommendation: Investigate cooling system efficiency
      poor: "For example, do something like this"
      not_applicable: "Some prompts don't need examples (simple Q&A, basic instructions)"
    quality_threshold: "Concrete, realistic, demonstrative (or marked N/A with justification)"

  validation_process:
    step_1_component_presence:
    - "Check each RICCE component is present"
    - "Score: Present (1), Weak (0.5), Missing (0)"

    step_2_component_quality:
    - "Assess quality on 1-10 scale per component"
    - "Use quality thresholds from definitions above"

    step_3_overall_assessment:
    - "Presence: 5/5 required for pass (or 4/5 if Examples N/A)"
    - "Quality: â‰¥7/10 per component for pass"
    - "If below threshold: document specific improvements needed"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# COGNITIVE RIGOR TECHNIQUES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

cognitive_rigor_techniques:
  purpose: Apply structured thinking methods to challenge assumptions and enhance quality
  timing: "Applied during Phase E (Engineer), Round 4"
  total_time: "8-11 minutes of focused analysis"

  technique_1_perspective_inversion:
    duration: "2-3 minutes"
    focus: "View prompt from AI's interpretation angle"
    questions:
    - "How will the AI parse this instruction? What's ambiguous from its perspective?"
    - "What assumptions might the AI make that differ from my intent?"
    - "Where could the AI reasonably interpret this in multiple ways?"
    - "What information is implicit to me but unknown to the AI?"
    outputs:
    - "List of potential misinterpretations"
    - "Ambiguous phrases needing clarification"
    - "Implicit assumptions to make explicit"

  technique_2_constraint_reversal:
    duration: "1-2 minutes"
    focus: "Question assumed constraints and requirements"
    questions:
    - "What if we removed this constraint? Is it truly necessary?"
    - "What constraints are missing that should be present?"
    - "Are any constraints contradictory or mutually exclusive?"
    - "What happens if constraints are violated? Are boundaries clear?"
    outputs:
    - "Unnecessary constraints to remove"
    - "Missing constraints to add"
    - "Contradictions to resolve"

  technique_3_assumption_audit:
    duration: "2 minutes"
    focus: "Challenge unstated assumptions and domain knowledge"
    questions:
    - "What domain knowledge am I assuming the AI has?"
    - "What unstated assumptions exist about context or process?"
    - "What would a complete novice need to know to follow this?"
    - "What cultural, technical, or domain-specific context is implicit?"
    outputs:
    - "Implicit assumptions to make explicit"
    - "Domain knowledge to include in context"
    - "Unstated prerequisites to clarify"

  technique_4_mechanism_first:
    duration: "3-4 minutes"
    focus: "Map AI's processing logic step-by-step"
    questions:
    - "What is the first thing the AI will process?"
    - "What information does it need at each step?"
    - "Where might processing get stuck or confused?"
    - "What's the reasoning chain from input to output?"
    outputs:
    - "Processing bottlenecks identified"
    - "Information dependencies mapped"
    - "Reasoning chain made explicit in prompt structure"

  application_process:
    when_to_apply: "During Engineer phase (Round 4), after framework selection, before restructuring"
    how_to_apply:
    - "Set timer for each technique (stay time-boxed)"
    - "Work through questions methodically"
    - "Document all insights (even if seemingly minor)"
    - "Prioritize insights by impact on clarity and specificity"
    - "Incorporate findings into Round 5 restructuring"
    output_format: |
      Cognitive Rigor Summary:
      - Perspective Inversion: [Key insight 1-2 sentences]
      - Constraint Reversal: [Key insight 1-2 sentences]
      - Assumption Audit: [Key insight 1-2 sentences]
      - Mechanism First: [Key insight 1-2 sentences]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MULTI-PERSPECTIVE ANALYSIS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

multi_perspective_analysis:
  purpose: Examine prompt from multiple viewpoints to identify blind spots
  minimum_perspectives: 3
  target_perspectives: 5
  timing: "Applied during Phase D (Discover), Round 2"

  core_perspectives:
    perspective_1_prompt_engineering:
      focus: "Technical quality of prompt construction"
      questions:
      - "Does this follow prompt engineering best practices?"
      - "Is the structure optimal for this task type?"
      - "Are there proven patterns we should apply?"
      outputs: "Framework recommendations, structural improvements"

    perspective_2_ai_interpretation:
      focus: "How AI models will parse and process this"
      questions:
      - "What tokens or patterns will the model focus on?"
      - "Where might attention mechanisms struggle?"
      - "What's the likely reasoning path?"
      outputs: "Ambiguity identification, processing concerns"

    perspective_3_user_clarity:
      focus: "Readability and understandability for humans"
      questions:
      - "Can a non-expert understand what's being asked?"
      - "Is the language clear and professional?"
      - "Would I understand this if I saw it in 6 months?"
      outputs: "Clarity improvements, jargon reduction"

  additional_perspectives:
    perspective_4_domain_expertise:
      focus: "Technical accuracy and domain appropriateness"
      questions:
      - "Is domain terminology used correctly?"
      - "Are domain-specific requirements captured?"
      - "What domain knowledge is assumed vs explicit?"
      outputs: "Technical corrections, domain context additions"

    perspective_5_usability:
      focus: "Practical effectiveness for intended use case"
      questions:
      - "Will this actually solve the user's problem?"
      - "Is this practical to implement/execute?"
      - "What edge cases or failure modes exist?"
      outputs: "Practical improvements, edge case handling"

  analysis_process:
    step_1_select_perspectives:
    - "Always include: Prompt Engineering, AI Interpretation, User Clarity"
    - "Add 1-2 more based on prompt complexity and domain"

    step_2_systematic_examination:
    - "Work through each perspective's questions"
    - "Document findings specific to that viewpoint"
    - "Note contradictions between perspectives"

    step_3_synthesize_insights:
    - "Identify patterns across perspectives"
    - "Prioritize findings by impact and frequency"
    - "Resolve contradictions with reasoned tradeoffs"

    output_format: |
      Multi-Perspective Analysis:
      - Prompt Engineering: [Finding]
      - AI Interpretation: [Finding]
      - User Clarity: [Finding]
      - [Optional 4th]: [Finding]
      - [Optional 5th]: [Finding]

      Cross-cutting insights: [Synthesis]
      Priority improvements: [Top 3]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# QUALITY STANDARDS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

quality_standards:
  overall_targets:
  - clear_score_minimum: 40_out_of_50
  - clear_score_per_dimension: 8_out_of_10_or_12_out_of_15_for_expression
  - improvement_delta_minimum: 5_points_increase
  - ricce_completeness: all_5_components_present_or_4_if_examples_na
  - ricce_quality: 7_out_of_10_per_component

  quick_mode_targets:
  - clear_score_minimum: 35_out_of_50
  - improvement_delta_minimum: any_positive_improvement
  - ricce_completeness: at_least_4_components
  - processing_time: under_10_seconds

  validation_gates:
    gate_1_ricce_completeness:
      check: "All RICCE components present (or 4/5 if Examples N/A)"
      action_if_fail: "Identify missing components, add in Harmonize phase"

    gate_2_clear_threshold:
      check: "Enhanced CLEAR score â‰¥40/50 and â‰¥8/10 per dimension"
      action_if_fail: "Identify weak dimensions, apply targeted improvements"

    gate_3_improvement_delta:
      check: "Enhanced score > Original score + 5 points"
      action_if_fail: "Re-examine enhancements, apply cognitive rigor techniques again"

    gate_4_expression_quality:
      check: "Expression dimension â‰¥12/15 (vague language removed)"
      action_if_fail: "Find and replace vague terms, increase specificity"

  fail_handling:
    if_gates_fail_after_harmonize:
    - "Document which gates failed and by how much"
    - "Present user with options: (A) one more refinement round, (B) accept current, (C) cancel"
    - "If user chooses A: Focus on failed gates only, apply targeted fixes"
    - "If user chooses B: Proceed with current version, note limitations in output"
    - "If user chooses C: Return STATUS=CANCELLED with summary of attempts"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TWO-LAYER TRANSPARENCY MODEL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

two_layer_transparency:
  purpose: "Balance complete analysis rigor with user-friendly progress updates"

  internal_layer:
    description: "Full DEPTH rigor, detailed analysis, comprehensive scoring"
    contents:
    - "Complete 10-round DEPTH execution"
    - "Multi-perspective analysis (3-5 perspectives)"
    - "Cognitive rigor technique application (4 techniques, 8-11 min)"
    - "Detailed RICCE validation"
    - "Comprehensive CLEAR scoring with dimension breakdowns"
    - "Framework selection rationale with alternatives considered"
    - "Assumption audits and mechanism mapping"
    visibility: "Executed fully, documented internally, included in final output report"

  external_layer:
    description: "Concise progress updates showing key decisions and phase completion"
    contents:
    - "Phase progress indicators (Phase D: Rounds 1-2/10)"
    - "Key decisions (Complexity: 7/10 â†’ Framework: CRAFT)"
    - "Quality checkpoints (CLEAR: 42/50 âœ“)"
    - "Major insights from cognitive rigor (1-2 sentences per technique)"
    - "Framework selection with brief rationale"
    visibility: "Shown to user in real-time during processing"
    format: |
      ğŸ” Phase D: Discovering prompt structure... [Rounds 1-2/10]
         Complexity: [X]/10 | Perspectives: [N]

      ğŸ”§ Phase E: Engineering framework ([Framework])... [Rounds 3-5/10]
         Cognitive rigor applied | [Key insight]

      ğŸ“ Phase P: Prototyping enhanced prompt... [Rounds 6-7/10]
         RICCE validation: [X]/5 components

      âœ… Phase T: Testing quality (CLEAR: [X]/50)... [Rounds 8-9/10]
         Target [met âœ“ | not met, gap: Y points]

      ğŸ¯ Phase H: Harmonizing final version... [Round 10/10]
         Final polish applied

  balance_principle: "Do ALL the rigorous work internally, show CONCISE progress externally"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MODE-SPECIFIC ADAPTATIONS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

mode_adaptations:
  auto_mode:
    description: "Automatic mode detection based on baseline CLEAR score"
    detection_algorithm:
      step_1_baseline_score: "Calculate CLEAR score for original prompt"
      step_2_mode_selection: |
        If baseline_score <= 25: mode = "quick"
        If baseline_score >= 26 AND baseline_score <= 35: mode = "improve"
        If baseline_score >= 36: mode = "refine"
      step_3_user_notification: |
        Display: "ğŸ“Š Auto-selected '{mode}' mode (current quality: {baseline_score}/50)"
        Include: Rationale for selection
        Offer: Override instructions for manual mode selection
    thresholds:
      poor: "0-25 (inclusive) â†’ quick mode"
      fair: "26-35 (inclusive) â†’ improve mode"
      good: "36-50 (inclusive) â†’ refine mode"
    boundary_examples:
      score_25: "quick mode (poor quality, needs rapid improvement)"
      score_26: "improve mode (fair quality, needs comprehensive enhancement)"
      score_35: "improve mode (fair quality approaching good)"
      score_36: "refine mode (good quality, needs polish)"

  quick_mode:
    modifications:
    - rounds: "1-5 (adaptive based on complexity)"
    - framework_selection: "automatic (no user prompts)"
    - cognitive_rigor: "abbreviated (1-2 techniques, 2-3 min total)"
    - multi_perspective: "minimum only (3 perspectives)"
    - clear_scoring: "simplified (faster calculation)"
    - iteration_menu: "skip (no Phase I - direct completion)"
    targets:
    - processing_time: "under 10 seconds"
    - quality_threshold: "35/50 (any improvement acceptable)"
    use_case: "Rapid iteration, testing, simple prompts, or low-quality baseline"

  improve_mode:
    modifications:
    - rounds: "full 10 rounds"
    - framework_selection: "automatic_with_explanation (no user prompts)"
    - cognitive_rigor: "full (4 techniques, 8-11 min)"
    - multi_perspective: "target 5 perspectives"
    - clear_scoring: "comprehensive"
    targets:
    - processing_time: "under 30 seconds"
    - quality_threshold: "40/50"
    use_case: "Standard enhancement workflow, moderate baseline quality"

  refine_mode:
    modifications:
    - rounds: "full 10 rounds"
    - framework_selection: "preserve existing (no change)"
    - cognitive_rigor: "focused on language precision"
    - focus_dimensions: "Expression (30% weight), Clarity, Specificity"
    - multi_perspective: "User Clarity + AI Interpretation primary"
    targets:
    - processing_time: "under 30 seconds"
    - quality_threshold: "43/50 (polish to high quality)"
    use_case: "Polish existing good prompts (36+ baseline score)"

  dimension_refinement_mode:
    trigger: "User selects 'Refine specific dimension' from iteration menu"
    modifications:
    - rounds: "3-5 focused rounds"
    - target_dimension: "user_selected (Expression, Logic, Reusability, etc.)"
    - framework_selection: "preserve existing"
    - focus_area: "narrow to single CLEAR dimension"

    workflow:
      step_1_identify_weakness:
      - "Show current score for target dimension"
      - "Explain why score is low"
      - "Identify specific issues (vague language, poor structure, etc.)"

      step_2_targeted_improvement:
      - "Apply dimension-specific techniques"
      - "Expression: Remove vague terms, increase specificity"
      - "Logic: Improve structure, clarify reasoning flow"
      - "Reusability: Make components modular, generalize"
      - "Correctness: Fix terminology, validate accuracy"
      - "Arrangement: Reorganize sections, improve hierarchy"

      step_3_validate_improvement:
      - "Recalculate CLEAR score for dimension"
      - "Show before/after comparison"
      - "Return to iteration menu or exit"

    targets:
    - dimension_score_improvement: "+1 to +3 points"
    - processing_time: "under 15 seconds"
    - preservation: "other dimensions unchanged"

  comparison_mode:
    trigger: ":compare flag in arguments OR 'Compare frameworks' from iteration menu"
    modifications:
    - rounds: "10 rounds per framework"
    - frameworks: "3 frameworks (RCAF, COSTAR, CRAFT)"
    - parallel_execution: "true (if resources allow)"
    - output_format: "comparison_table"

    workflow:
      step_1_select_frameworks:
      - "Auto-select 3 frameworks based on complexity"
      - "Low (1-4): RCAF, RACE, CIDI"
      - "Medium (5-6): RCAF, COSTAR, TIDD-EC"
      - "High (7-10): RCAF, COSTAR, CRAFT"

      step_2_parallel_enhancement:
      - "Run DEPTH framework for each"
      - "Calculate CLEAR scores independently"
      - "Generate 3 complete enhanced prompts"

      step_3_comparison_table:
      - "Show side-by-side comparison"
      - "CLEAR scores, length, preview"
      - "Best-for descriptions"
      - "Recommendation based on highest score"

      step_4_user_selection:
      - "Ask user to select framework"
      - "Return selected enhanced prompt"
      - "Save to file with framework name"

    targets:
    - processing_time: "under 60 seconds total"
    - frameworks_compared: "3"
    - recommendation_accuracy: "highest CLEAR score"

  learning_mode:
    trigger: ":explain flag in arguments"
    modifications:
    - rounds: "2 rounds only (Discovery phase)"
    - enhancement: "false (analysis only, no changes)"
    - educational_output: "true (explain what's wrong)"
    - user_interaction: "full explanation then ask to proceed"

    workflow:
      step_1_analyze_prompt:
      - "Perform Discovery phase (RICCE analysis)"
      - "Calculate baseline CLEAR score"
      - "Identify all gaps and weaknesses"

      step_2_explain_issues:
      - "Show what's missing (RICCE checklist)"
      - "Explain why each missing component matters"
      - "Show how AI might misinterpret current prompt"
      - "Estimate impact of each gap (high/medium/low)"

      step_3_offer_enhancement:
      - "Ask: 'Ready to enhance? [y/n]'"
      - "If yes: Proceed with full DEPTH workflow"
      - "If no: Save analysis to file, exit"

    outputs:
    - analysis_report: "Educational breakdown of issues"
    - ricce_gaps: "Missing components with explanations"
    - clear_score_explanation: "Why score is low, dimension by dimension"
    - fix_suggestions: "What would improve each dimension"

  test_mode:
    trigger: ":test flag with --sample argument"
    modifications:
    - rounds: "full 10 rounds (enhance first)"
    - testing: "true (run sample through both prompts)"
    - comparison: "original_output vs enhanced_output"
    - user_interaction: "show results, ask to proceed or retry"

    workflow:
      step_1_enhance_prompt:
      - "Run full DEPTH enhancement workflow"
      - "Generate enhanced prompt"
      - "Calculate CLEAR score"

      step_2_extract_sample:
      - "Parse --sample argument from command"
      - "If no sample: Ask user for sample input"
      - "Validate sample is appropriate for prompt type"

      step_3_test_original:
      - "Create test context with original prompt"
      - "Process sample input"
      - "Capture output (simulated or actual)"
      - "Note: May require using Task tool with sample"

      step_4_test_enhanced:
      - "Create test context with enhanced prompt"
      - "Process same sample input"
      - "Capture output"

      step_5_compare_outputs:
      - "Show side-by-side: original output vs enhanced output"
      - "Highlight differences in quality"
      - "Explain why enhanced is better (specificity, structure, etc.)"
      - "Show CLEAR score correlation to output quality"

      step_6_user_decision:
      - "Ask: Save enhanced prompt? [y/n]"
      - "If yes: Save and exit"
      - "If no: Offer to refine further"

    outputs:
    - test_results_comparison: "side_by_side_output_quality"
    - quality_correlation: "clear_score_impact_on_output"
    - user_verdict: "proceed_or_retry"

  audit_mode:
    trigger: ":audit flag (no prompt text required)"
    modifications:
    - scanning: "true (scan .claude/commands/ and .claude/commands/*/assets/)"
    - analysis: "calculate CLEAR scores for all prompts"
    - recommendations: "prioritize by score (lowest first)"
    - user_interaction: "show results, offer batch enhancement"

    workflow:
      step_1_scan_directories:
      - "Glob search: .claude/commands/*.md"
      - "Glob search: .claude/commands/*/assets/*.yaml"
      - "Glob search: .claude/commands/**/*.md"
      - "Extract prompt content from each file"

      step_2_analyze_prompts:
      - "For each prompt: Calculate baseline CLEAR score"
      - "Categorize: Poor (0-25), Fair (26-35), Good (36-40), Excellent (41+)"
      - "Identify missing RICCE components"
      - "Estimate enhancement potential"

      step_3_generate_report:
      - "Group by quality category"
      - "Sort by score (lowest first = highest priority)"
      - "Show file path, current score, issues, recommended action"

      step_4_offer_batch_enhancement:
      - "Ask: Enhance all low-quality prompts? [y/n/select]"
      - "If yes: Queue all Poor + Fair prompts for enhancement"
      - "If select: Let user pick which to enhance"
      - "Process sequentially with progress updates"

    outputs:
    - audit_report: "quality_breakdown_by_file"
    - priority_list: "sorted_by_score_ascending"
    - batch_enhancement_queue: "if_user_accepts"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RULES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

rules:
  ALWAYS:
  - Execute all 10 DEPTH rounds unless quick mode (no skipping)
  - Calculate CLEAR scores for both original and enhanced prompts
  - Validate RICCE completeness before finalizing
  - Apply framework selection algorithm based on complexity
  - Use cognitive rigor techniques during Engineer phase
  - Perform multi-perspective analysis (minimum 3 perspectives)
  - Document framework selection rationale
  - Show two-layer transparency (internal rigor, external conciseness)
  - Remove vague language (good, better, properly, carefully, thoroughly)
  - Make implicit assumptions explicit
  - Include concrete examples where applicable
  - Use specific, measurable, actionable language

  NEVER:
  - Skip DEPTH phases or rounds (except in quick mode)
  - Fabricate quality scores (calculate rigorously)
  - Use vague adjectives in enhanced prompts
  - Leave ambiguous instructions or constraints
  - Skip RICCE validation
  - Ignore cognitive rigor insights
  - Proceed with score <40 without user confirmation
  - Change framework mid-process without rationale
  - Leave assumptions implicit
  - Use placeholder text in final enhanced prompt

  PREFER:
  - Specific over vague (250 words â†’ not "brief")
  - Measurable over subjective (â‰¥40/50 â†’ not "good quality")
  - Explicit over implicit (make assumptions visible)
  - Structured over unstructured (framework application)
  - Concrete over abstract (examples and demonstrations)
  - Simple over complex (RCAF over CRAFT when complexity allows)
  - Active voice over passive (the AI will X â†’ X will be done)
  - Short sentences over long (readability)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ERROR HANDLING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

error_handling:
  scenario_1_complexity_assessment_fails:
    detection: "Unable to calculate complexity score (missing information)"
    recovery:
    - "Default to complexity 5 (moderate)"
    - "Use RCAF framework (safest choice)"
    - "Note assumption in output"
    - "Continue processing"

  scenario_2_framework_selection_timeout:
    detection: "User doesn't respond to framework choice prompt within reasonable time"
    recovery:
    - "Default to RCAF (most versatile)"
    - "Notify user of fallback decision"
    - "Continue processing"
    - "Note in output: Framework chosen by default"

  scenario_3_clear_scoring_ambiguous:
    detection: "Dimension score unclear (between two ranges)"
    recovery:
    - "Score conservatively (lower of two options)"
    - "Document uncertainty in scoring notes"
    - "Provide reasoning for score choice"
    - "Continue processing"

  scenario_4_ricce_component_not_applicable:
    detection: "Examples component truly not applicable (e.g., simple Q&A)"
    recovery:
    - "Mark Examples as N/A with justification"
    - "Accept 4/5 RICCE completeness"
    - "Ensure other 4 components are high quality"
    - "Note in validation results"

  scenario_5_enhancement_worse_than_original:
    detection: "Enhanced CLEAR score < Original CLEAR score"
    recovery:
    - "Re-run Harmonize phase (Round 10)"
    - "Focus on dimensions that decreased"
    - "Apply constraint reversal (removed necessary element?)"
    - "If still worse: Return original with analysis of why enhancement failed"

  scenario_6_processing_timeout:
    detection: "DEPTH processing exceeds 30 seconds (or 10s for quick mode)"
    recovery:
    - "Complete current round, skip remaining rounds"
    - "Use partial results to generate enhanced prompt"
    - "Note incomplete processing in output"
    - "Offer to retry with :quick mode"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OUTPUT SPECIFICATIONS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

output_specifications:
  dual_output_system:
    purpose: "Separate analysis from prompt for clarity and reusability"
    rationale: |
      - Analysis document = human-readable assessment for review
      - YAML prompt = machine-readable format for direct workflow integration
      - Separation enables independent use of each file
      - YAML format allows easy import into other tools/workflows

    file_1_analysis:
      filename: "prompt_analysis.md"
      purpose: "Human-readable quality assessment and enhancement report"
      format: "Markdown"

    file_2_prompt:
      filename: "enhanced_prompt.yaml"
      purpose: "Machine-readable enhanced prompt for direct use"
      format: "YAML with framework-specific structure"

  enhanced_prompt_yaml:
    format: "YAML with framework-specific components"
    structure: |
      ---
      # Enhanced Prompt - YAML Format
      # Framework: [RCAF|COSTAR|CRAFT|etc.]

      metadata:
        framework: "[framework_name]"
        complexity: [1-10]
        clear_score: [0-50]
        generated: "[ISO 8601 timestamp]"
        mode: "[quick|improve|refine|interactive]"
        original_score: [0-50]
        improvement: [delta]

      prompt:
        # Framework-specific components (each framework has its own complete structure)
        # Example shown below is RCAF framework structure
        # See framework_specific_components in step_15 for all framework structures

        # IMPORTANT: These are PLACEHOLDER EXAMPLES only
        # The actual YAML file MUST contain real enhanced prompt content
        # NEVER leave placeholder text in generated output

        role: |
          {ACTUAL_ROLE_CONTENT_GOES_HERE}
        context: |
          {ACTUAL_CONTEXT_CONTENT_GOES_HERE}
        action: |
          {ACTUAL_ACTION_STEPS_GO_HERE}
        format: |
          {ACTUAL_OUTPUT_FORMAT_GOES_HERE}

        # NOTE: Different frameworks have different components
        # - CRAFT uses: context, role, action, format, target
        # - COSTAR uses: context, objective, style, tone, audience, response
        # - RACE uses: role, action, context, examples
        # - CIDI uses: context, instructions, details, input
        # - TIDD_EC uses: task, instructions, details, deliverables, examples, constraints
        # - CRISPE uses: capacity, role, insight, statement, personality, experiment
        # See framework_mapping and framework_definitions for complete component lists

    framework_mapping:
      RCAF: [ "role", "context", "action", "format" ]
      COSTAR: [ "context", "objective", "style", "tone", "audience", "response" ]
      RACE: [ "role", "action", "context", "examples" ]
      CIDI: [ "context", "instructions", "details", "input" ]
      TIDD_EC: [ "task", "instructions", "details", "deliverables", "examples", "constraints" ]
      CRISPE: [ "capacity", "role", "insight", "statement", "personality", "experiment" ]
      CRAFT: [ "context", "role", "action", "format", "target" ]

  prompt_analysis_md:
    format: "Structured markdown with quality assessment and enhancement report"
    sections:
    - "Header (metadata: generated, scores, framework)"
    - "Quality Assessment (CLEAR scores, comparison table, strong/weak areas)"
    - "RICCE Completeness (checklist with descriptions)"
    - "Framework Selection Rationale (complexity, choice, alternatives)"
    - "Cognitive Rigor Summary (4 technique insights)"
    - "Key Improvements (top 3-5 with explanations)"
    - "Original Prompt Reference (for comparison)"
    - "Processing Metadata (mode, rounds, time)"

    template_structure: |
      # Prompt Enhancement Analysis

      **Generated:** [timestamp]
      **Original CLEAR Score:** [X]/50 ([quality_label])
      **Enhanced CLEAR Score:** [Y]/50 ([quality_label])
      **Improvement:** +[Z] points ([P]% better)
      **Framework Applied:** [framework_name]

      ---

      ## Quality Assessment

      ### CLEAR Score Comparison

      | Dimension            | Original   | Enhanced   | Delta    |
      | -------------------- | ---------- | ---------- | -------- |
      | **Correctness** (10) | [X]/10     | [Y]/10     | +[Z]     |
      | **Logic** (10)       | [X]/10     | [Y]/10     | +[Z]     |
      | **Expression** (15)  | [X]/15     | [Y]/15     | +[Z]     |
      | **Arrangement** (10) | [X]/10     | [Y]/10     | +[Z]     |
      | **Reusability** (5)  | [X]/5      | [Y]/5      | +[Z]     |
      | **TOTAL**            | **[X]/50** | **[Y]/50** | **+[Z]** |

      **Target Met:** [YES âœ… / NO âŒ] (Target: â‰¥40/50)

      ### Strong Areas
      â€¢ [Dimension]: [Score]/[Max] ([interpretation])
      â€¢ [Dimension]: [Score]/[Max] ([interpretation])

      ### Needs Work
      â€¢ [Dimension]: [Score]/[Max] ([specific issue])

      ### What This Means
      [Plain language explanation of quality level and practical impact]

      ---

      ## RICCE Completeness

      - âœ… **Role**: [description]
      - âœ… **Instructions**: [description]
      - âœ… **Context**: [description]
      - âœ… **Constraints**: [description]
      - âœ… **Examples**: [description or N/A]

      ---

      ## Framework Selection Rationale

      **Complexity Assessment:** [X]/10
      - [Brief explanation of complexity factors]

      **Framework Choice:** [framework_name]
      - [Rationale for selection]
      - [Alternative frameworks considered]

      ---

      ## Cognitive Rigor Applied

      - **Perspective Inversion**: [key insight]
      - **Constraint Reversal**: [key insight]
      - **Assumption Audit**: [key insight]
      - **Mechanism First**: [key insight]

      ---

      ## Key Improvements

      1. **[Category]**: [Description with rationale]
      2. **[Category]**: [Description with rationale]
      3. **[Category]**: [Description with rationale]

      **Characters:** Original [XXX] â†’ Enhanced [YYY] ([+/-ZZZ])

      ---

      ## Original Prompt (Reference)

      ```
      [Original prompt text for comparison]
      ```

      ---

      ## Processing Metadata

      - **Mode:** [quick|improve|refine|interactive]
      - **DEPTH Rounds:** [N]/10
      - **Processing Time:** [X.X] seconds
      - **Generated:** [ISO 8601 timestamp]

      ---

      **Enhanced prompt available in:** `enhanced_prompt.yaml`
      **Import into workflows:** Load YAML directly for automated use

    quality_label_mapping:
      poor: "0-25"
      fair: "26-35"
      good: "36-40"
      very_good: "41-45"
      excellent: "46-50"

  file_locations:
    determine_output_path:
      check_priority_1: "Active spec folder (.claude/.spec-active.$$)"
      check_priority_2: "Legacy spec folder (.claude/.spec-active)"
      check_priority_3: "/export/ directory with sequential numbering"

    naming_convention:
      analysis_file: "prompt_analysis.md"
      prompt_file: "enhanced_prompt.yaml"

    output_paths:
      if_spec_active: |
        analysis: {spec_folder_path}/prompt_analysis.md
        prompt: {spec_folder_path}/enhanced_prompt.yaml
      if_no_spec: |
        analysis: /export/[###]-prompt-analysis-[timestamp].md
        prompt: /export/[###]-enhanced-prompt-[timestamp].yaml

  metadata:
    include:
    - "Processing timestamp (ISO 8601)"
    - "Mode used (quick, improve, refine, interactive)"
    - "Framework selected (RCAF, COSTAR, CRAFT, etc.)"
    - "Complexity score (1-10)"
    - "DEPTH rounds completed (1-10)"
    - "Processing time (seconds)"
    - "Original prompt (for reference)"
    - "Character count (original vs enhanced)"
    - "CLEAR scores (original, enhanced, delta)"
    - "RICCE completeness (5/5 or 4/5 if N/A)"

    contextual_interpretation_guidelines:
    - "Map score to quality label using quality_label_mapping"
    - "Calculate percentage improvement: ((enhanced - original) / original) * 100"
    - "Identify top 2-3 highest-scoring dimensions as 'Strong areas'"
    - "Identify dimensions below threshold (8/10 or 12/15) as 'Needs work'"
    - "Write plain language explanation covering:"
    - "  - What the score means in practical terms"
    - "  - What changed between original and enhanced"
    - "  - Why those changes matter for AI interpretation"
    - "  - What the user can expect from the enhanced prompt"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WORKFLOW EXECUTION NOTES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

execution_notes:
  dual_output_generation:
  - "Generate TWO files instead of one"
  - "File 1: prompt_analysis.md (human-readable assessment)"
  - "File 2: enhanced_prompt.yaml (machine-readable prompt)"
  - "Both files go to same location (spec folder or /export/)"
  - "Write files sequentially: analysis first, then YAML"
  - "Both files required for STATUS=OK (fail if either write fails)"
  phase_progression:
  - "Phases MUST execute sequentially (D â†’ E â†’ P â†’ T â†’ H)"
  - "Rounds within phases execute in order (no skipping)"
  - "Each phase builds on previous phase outputs"
  - "User interactions block progress until response received"

  cognitive_rigor_timing:
  - "Total time budget: 8-11 minutes"
  - "Stay within time boxes (focus on insights, not perfection)"
  - "Document all insights even if minor"
  - "Apply insights during Round 5 restructuring"

  quality_validation:
  - "Calculate scores after Round 8 (Test phase)"
  - "If below threshold: Prepare targeted improvements for Round 10"
  - "If still below after Round 10: Prompt user for decision"
  - "Never proceed to output with <40/50 without user confirmation"

  transparency_delivery:
  - "Show external layer updates in real-time"
  - "Keep updates concise (1-2 lines per phase)"
  - "Include key metrics (complexity, score, rounds)"
  - "Save internal layer details for final report"

  framework_application:
  - "Apply framework structure explicitly (visible headers/sections)"
  - "Ensure each component is substantive (not placeholder)"
  - "Use framework terminology in enhanced prompt"
  - "Make structure self-documenting"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PHASE 6: DUAL OUTPUT GENERATION (File Creation Workflow)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

phase_6_dual_output_generation:
  phase_name: "Dual Output Generation"
  emoji: "ğŸ“„"
  purpose: "Create both analysis markdown and YAML prompt files"
  trigger: "After Phase H (Harmonize) completes successfully"

  steps:
    step_13_determine_output_location:
      action: "Determine where to save both output files"
      priority_order:
        1: "Check for active spec folder (.claude/.spec-active.$$ or .claude/.spec-active)"
        2: "If spec folder exists, use: {spec_folder_path}/"
        3: "If no spec folder, use: /export/ with sequential numbering"
      outputs:
      - base_path: "Directory path where both files will be saved"
      - analysis_file_path: "{base_path}/prompt_analysis.md"
      - prompt_file_path: "{base_path}/enhanced_prompt.yaml"

      file_naming:
        with_spec_folder:
          analysis: "prompt_analysis.md"
          prompt: "enhanced_prompt.yaml"
        without_spec_folder:
          analysis: "[###]-prompt-analysis-[YYYYMMDD-HHMMSS].md"
          prompt: "[###]-enhanced-prompt-[YYYYMMDD-HHMMSS].yaml"
          sequential_number: "Find highest existing number in /export/, increment by 1"

    step_14_assemble_analysis_file:
      action: "Assemble complete prompt_analysis.md content"
      tool: "Construct in memory (Write tool in step 16)"

      content_structure:
        header:
        - "# Prompt Enhancement Analysis"
        - "**Generated:** {ISO_8601_timestamp}"
        - "**Original CLEAR Score:** {original_score}/50 ({quality_label})"
        - "**Enhanced CLEAR Score:** {enhanced_score}/50 ({quality_label})"
        - "**Improvement:** +{delta} points ({percentage}% better)"
        - "**Framework Applied:** {framework_name}"
        - "---"

        section_1_quality_assessment:
        - "## Quality Assessment"
        - "### CLEAR Score Comparison"
        - "{CLEAR_comparison_table}"
        - "**Target Met:** {YES/NO} (Target: â‰¥40/50)"
        - "### Strong Areas"
        - "{List dimensions scoring â‰¥8/10 or â‰¥12/15}"
        - "### Needs Work"
        - "{List dimensions below threshold or N/A if none}"
        - "### What This Means"
        - "{Plain language explanation of quality impact}"

        section_2_ricce_completeness:
        - "## RICCE Completeness"
        - "- âœ… **Role**: {description_of_role_component}"
        - "- âœ… **Instructions**: {description_of_instructions}"
        - "- âœ… **Context**: {description_of_context}"
        - "- âœ… **Constraints**: {description_of_constraints}"
        - "- âœ… **Examples**: {description_or_NA_with_justification}"

        section_3_framework_rationale:
        - "## Framework Selection Rationale"
        - "**Complexity Assessment:** {complexity_score}/10"
        - "{Brief explanation of complexity factors}"
        - "**Framework Choice:** {framework_name}"
        - "{Rationale for selection}"
        - "{Alternative frameworks considered}"

        section_4_cognitive_rigor:
        - "## Cognitive Rigor Applied"
        - "- **Perspective Inversion**: {key_insight}"
        - "- **Constraint Reversal**: {key_insight}"
        - "- **Assumption Audit**: {key_insight}"
        - "- **Mechanism First**: {key_insight}"

        section_5_key_improvements:
        - "## Key Improvements"
        - "1. **{Category}**: {Description with rationale}"
        - "2. **{Category}**: {Description with rationale}"
        - "3. **{Category}**: {Description with rationale}"
        - "(List 3-5 most impactful changes)"
        - "**Characters:** Original {XXX} â†’ Enhanced {YYY} ({+/-ZZZ})"

        section_6_original_reference:
        - "## Original Prompt (Reference)"
        - "```"
        - "{original_prompt_text}"
        - "```"

        section_7_metadata:
        - "## Processing Metadata"
        - "- **Mode:** {quick|improve|refine|interactive}"
        - "- **DEPTH Rounds:** {N}/10"
        - "- **Processing Time:** {X.X} seconds"
        - "- **Generated:** {ISO_8601_timestamp}"

        footer:
        - "---"
        - "**Enhanced prompt available in:** `enhanced_prompt.yaml`"
        - "**Import into workflows:** Load YAML directly for automated use"

      validation:
      - "Verify all placeholders replaced with actual values"
      - "Confirm CLEAR scores match calculated values"
      - "Ensure markdown formatting is correct"
      - "Check that framework name is accurate"

    step_15_assemble_yaml_file:
      action: "Assemble complete enhanced_prompt.yaml content"
      tool: "Construct in memory (Write tool in step 16)"

      content_structure:
        header:
        - "---"
        - "# Enhanced Prompt - YAML Format"
        - "# Framework: {framework_name}"
        - ""

        metadata_section:
        - "metadata:"
        - "  framework: \"{framework_name}\""
        - "  complexity: {complexity_score}"
        - "  clear_score: {enhanced_score}"
        - "  generated: \"{ISO_8601_timestamp}\""
        - "  mode: \"{mode}\""
        - "  original_score: {original_score}"
        - "  improvement: {delta}"
        - ""

        prompt_section:
        - "prompt:"
        - "{framework_specific_components}"
        - ""

        usage_examples:
        - "# Usage Examples:"
        - "# Python: import yaml; prompt = yaml.safe_load(open('enhanced_prompt.yaml'))"
        - "# Node.js: const prompt = require('js-yaml').load(fs.readFileSync('enhanced_prompt.yaml'))"
        - "# Access: prompt['prompt']['role'] + prompt['prompt']['context'] + ..."

      framework_specific_components:
        RCAF:
        - "  role: |"
        - "    {ACTUAL_ROLE_CONTENT_HERE - DO NOT LEAVE PLACEHOLDER}"
        - "  context: |"
        - "    {ACTUAL_CONTEXT_CONTENT_HERE - DO NOT LEAVE PLACEHOLDER}"
        - "  action: |"
        - "    {ACTUAL_ACTION_CONTENT_HERE - DO NOT LEAVE PLACEHOLDER}"
        - "  format: |"
        - "    {ACTUAL_FORMAT_CONTENT_HERE - DO NOT LEAVE PLACEHOLDER}"

        COSTAR:
        - "  context: |"
        - "    {ACTUAL_CONTEXT_CONTENT}"
        - "  objective: |"
        - "    {ACTUAL_OBJECTIVE_CONTENT}"
        - "  style: |"
        - "    {ACTUAL_STYLE_CONTENT}"
        - "  tone: |"
        - "    {ACTUAL_TONE_CONTENT}"
        - "  audience: |"
        - "    {ACTUAL_AUDIENCE_CONTENT}"
        - "  response: |"
        - "    {ACTUAL_RESPONSE_FORMAT_CONTENT}"

        RACE:
        - "  role: |"
        - "    {ACTUAL_ROLE_CONTENT}"
        - "  action: |"
        - "    {ACTUAL_ACTION_CONTENT}"
        - "  context: |"
        - "    {ACTUAL_CONTEXT_CONTENT}"
        - "  examples: |"
        - "    {ACTUAL_EXAMPLES_CONTENT}"

        CIDI:
        - "  context: |"
        - "    {ACTUAL_CONTEXT_CONTENT}"
        - "  instructions: |"
        - "    {ACTUAL_INSTRUCTIONS_CONTENT}"
        - "  details: |"
        - "    {ACTUAL_DETAILS_CONTENT}"
        - "  input: |"
        - "    {ACTUAL_INPUT_CONTENT}"

        TIDD_EC:
        - "  task: |"
        - "    {ACTUAL_TASK_CONTENT}"
        - "  instructions: |"
        - "    {ACTUAL_INSTRUCTIONS_CONTENT}"
        - "  details: |"
        - "    {ACTUAL_DETAILS_CONTENT}"
        - "  deliverables: |"
        - "    {ACTUAL_DELIVERABLES_CONTENT}"
        - "  examples: |"
        - "    {ACTUAL_EXAMPLES_CONTENT}"
        - "  constraints: |"
        - "    {ACTUAL_CONSTRAINTS_CONTENT}"

        CRISPE:
        - "  capacity: |"
        - "    {ACTUAL_CAPACITY_CONTENT}"
        - "  role: |"
        - "    {ACTUAL_ROLE_CONTENT}"
        - "  insight: |"
        - "    {ACTUAL_INSIGHT_CONTENT}"
        - "  statement: |"
        - "    {ACTUAL_STATEMENT_CONTENT}"
        - "  personality: |"
        - "    {ACTUAL_PERSONALITY_CONTENT}"
        - "  experiment: |"
        - "    {ACTUAL_EXPERIMENT_CONTENT}"

        CRAFT:
        - "  context: |"
        - "    {ACTUAL_CONTEXT_CONTENT}"
        - "  role: |"
        - "    {ACTUAL_ROLE_CONTENT}"
        - "  action: |"
        - "    {ACTUAL_ACTION_CONTENT}"
        - "  format: |"
        - "    {ACTUAL_FORMAT_CONTENT}"
        - "  target: |"
        - "    {ACTUAL_TARGET_CONTENT}"

      critical_instructions:
      - "NEVER leave placeholder text in YAML output"
      - "Replace ALL {ACTUAL_*_CONTENT} with actual enhanced prompt content"
      - "Ensure multi-line strings use | (pipe) operator for readability"
      - "Validate YAML syntax before writing (no unclosed quotes, proper indentation)"
      - "Each component must contain substantive content from enhanced prompt"
      - "If a component is not applicable, still include with brief note (not empty)"

      validation:
      - "Verify all {ACTUAL_*_CONTENT} placeholders are replaced"
      - "Confirm YAML is valid (proper indentation, quotes, structure)"
      - "Check metadata values match analysis file"
      - "Ensure framework components match framework_mapping"
      - "Test parsability: yaml.safe_load() should succeed"

    step_16_write_both_files:
      action: "Write both files to determined location"
      tool: "Write tool (two separate invocations)"
      sequence: "sequential (not parallel - write analysis first, then YAML)"

      write_sequence:
        1_write_analysis:
          tool: "Write"
          file_path: "{analysis_file_path}"
          content: "{assembled_analysis_content_from_step_14}"
          verification: "Check Write tool returns success"

        2_write_yaml:
          tool: "Write"
          file_path: "{prompt_file_path}"
          content: "{assembled_yaml_content_from_step_15}"
          verification: "Check Write tool returns success"

      error_handling:
        if_analysis_write_fails:
        - "Log error: Analysis file write failed"
        - "Do NOT write YAML file"
        - "Return STATUS=ERROR with message"
        - "Do NOT proceed to step 17"

        if_yaml_write_fails_after_analysis_succeeds:
        - "Log error: YAML file write failed"
        - "Consider analysis file orphaned"
        - "Return STATUS=ERROR with message"
        - "Note: Analysis file exists but YAML is missing"
        - "User may need to manually delete analysis file or retry"

        atomic_write_guarantee:
        - "Both files MUST write successfully for STATUS=OK"
        - "If either fails, entire operation considered failed"
        - "No partial success state"

    step_17_report_success:
      action: "Report successful dual-output generation to user"
      display_format: |
        âœ… Enhanced prompt generated successfully!

        ğŸ“„ **Files created:**
        - Analysis: {analysis_file_path}
        - Prompt:   {prompt_file_path}

        ğŸ“Š **Quality:**
        - Original CLEAR Score: {original_score}/50
        - Enhanced CLEAR Score: {enhanced_score}/50
        - Improvement: +{delta} points ({percentage}% better)

        ğŸ”§ **Framework:** {framework_name}

        ğŸ’¡ **Next steps:**
        - Review analysis: `cat {analysis_file_path}`
        - Import YAML: `import yaml; prompt = yaml.safe_load(open('{prompt_file_path}'))`
        - Use enhanced prompt in your workflow

      status_return:
      - "STATUS=OK"
      - "ANALYSIS={analysis_file_path}"
      - "PROMPT={prompt_file_path}"
      - "SCORE={enhanced_score}/50"
      - "IMPROVEMENT=+{delta}"

      optional_actions:
        if_not_quick_mode:
        - "Present iteration menu (Phase I options)"
        if_quick_mode:
        - "Skip iteration menu, complete workflow"
