---
# DEPTH Framework Prompt Enhancement Workflow
# Integrates: Prompt Improver v0.922 + DEPTH Framework v0.106 + Interactive Mode v0.642
# Version: 1.0.0
# Purpose: Systematic prompt enhancement using multi-perspective analysis, framework selection, and quality scoring
---

framework:
  role: Prompt Engineering Specialist with Multi-Perspective Analysis Expertise
  purpose: Transform raw prompts into optimized, framework-structured prompts through systematic DEPTH methodology
  action: Apply 5-phase enhancement workflow with cognitive rigor, framework selection, RICCE validation, and CLEAR scoring

  operating_mode:
    workflow: sequential_with_cognitive_rigor
    workflow_compliance: MANDATORY
    workflow_execution: autonomous_with_user_checkpoints
    approvals: framework_selection_for_complexity_5_plus
    tracking: phase_round_progress_transparent
    validation: ricce_and_clear_scoring

# ==============================================================================
# DEPTH METHODOLOGY: 5 Phases, 10 Rounds
# ==============================================================================

depth_phases:
  overview: |
    DEPTH = Discover, Engineer, Prototype, Test, Harmonize
    Total rounds: 10 (or 1-5 for quick mode)
    Each phase builds on previous, no skipping allowed

  phase_d_discover:
    name: "Discover - Understand Intent and Structure"
    rounds: [1, 2]
    objectives:
      - Analyze prompt purpose, audience, and expected output
      - Identify missing RICCE components (Role, Instructions, Context, Constraints, Examples)
      - Assess complexity on 1-10 scale
      - Perform multi-perspective analysis (minimum 3 perspectives)
      - Establish baseline CLEAR score for original prompt

    activities:
      round_1_intent_analysis:
        focus: "What is this prompt trying to achieve?"
        tasks:
          - Parse prompt text for explicit and implicit goals
          - Identify target audience (AI model, specific use case)
          - Determine output type (text, code, analysis, creative, etc.)
          - Note ambiguous language or missing context
          - Assess current structure (unstructured, semi-structured, framework-based)
        outputs:
          - intent_statement: clear_articulation_of_prompt_purpose
          - audience_profile: target_user_and_ai_model_characteristics
          - output_type: expected_deliverable_format
          - ambiguities_list: unclear_or_missing_elements

      round_2_component_gap_analysis:
        focus: "What RICCE components are missing or weak?"
        tasks:
          - Check for Role definition (persona, expertise level)
          - Check for Instructions clarity (unambiguous steps)
          - Check for Context provision (relevant background)
          - Check for Constraints specification (boundaries, requirements)
          - Check for Examples inclusion (concrete demonstrations)
          - Assess complexity (1-10 scale using complexity_assessment formula)
          - Perform multi-perspective analysis (3-5 perspectives)
        outputs:
          - ricce_gaps: list_of_missing_or_weak_components
          - complexity_score: 1_to_10_scale
          - perspectives_analyzed: [prompt_engineering, ai_interpretation, user_clarity, domain_expertise, usability]
          - baseline_clear_score: original_prompt_quality_0_to_50

  phase_e_engineer:
    name: "Engineer - Apply Framework and Cognitive Rigor"
    rounds: [3, 4, 5]
    objectives:
      - Select appropriate framework based on complexity
      - Apply cognitive rigor techniques to challenge assumptions
      - Restructure prompt using selected framework
      - Enhance clarity, specificity, and completeness

    activities:
      round_3_framework_selection:
        focus: "Which framework best suits this prompt's complexity?"
        tasks:
          - Apply framework_selection_algorithm based on complexity_score
          - For complexity 5-6: Prepare user choice (RCAF, COSTAR, TIDD-EC)
          - For complexity 7: Prepare simplification choice (RCAF streamlined vs CRAFT comprehensive)
          - For complexity 8-10: Auto-select CRAFT, note TIDD-EC alternative
          - Document framework selection rationale
        outputs:
          - selected_framework: framework_name_with_rationale
          - framework_structure: component_list_for_selected_framework
          - user_interaction_required: yes_no_with_options_if_yes

      round_4_cognitive_rigor:
        focus: "Apply rigorous thinking techniques to enhance prompt"
        tasks:
          - Perspective Inversion (2-3 min): How will the AI interpret this? What assumptions might it make?
          - Constraint Reversal (1-2 min): What if we removed this constraint? Is it necessary?
          - Assumption Audit (2 min): What unstated assumptions exist? What domain knowledge is assumed?
          - Mechanism First (3-4 min): How will the AI process this step-by-step? What's the reasoning chain?
        outputs:
          - perspective_inversion_insights: key_revelations_from_ai_viewpoint
          - constraint_reversal_insights: unnecessary_or_missing_constraints
          - assumption_audit_insights: unstated_assumptions_to_make_explicit
          - mechanism_first_insights: ai_processing_logic_improvements

      round_5_restructuring:
        focus: "Restructure prompt using selected framework"
        tasks:
          - Map original prompt elements to framework components
          - Fill missing components identified in Discovery phase
          - Enhance clarity using cognitive rigor insights
          - Add specific instructions, context, and constraints
          - Incorporate domain-specific terminology where appropriate
        outputs:
          - restructured_prompt_draft: prompt_following_framework_structure
          - component_mapping: original_elements_to_framework_mapping
          - enhancements_applied: list_of_improvements_with_rationale

  phase_p_prototype:
    name: "Prototype - Generate and Validate Enhanced Version"
    rounds: [6, 7]
    objectives:
      - Generate complete enhanced prompt draft
      - Apply RICCE validation to ensure completeness
      - Refine language for maximum clarity and precision
      - Ensure all framework components are properly populated

    activities:
      round_6_draft_generation:
        focus: "Create complete enhanced prompt following framework"
        tasks:
          - Generate full prompt text with framework structure clearly visible
          - Ensure each framework component is substantive (not placeholder text)
          - Add concrete examples where applicable
          - Use precise language (avoid vague terms like "good", "better", "thorough")
          - Include measurable criteria where possible
        outputs:
          - enhanced_prompt_draft: complete_prompt_text
          - framework_compliance: verification_all_components_present
          - language_precision_score: 1_to_10_vagueness_assessment

      round_7_ricce_validation:
        focus: "Validate RICCE completeness and quality"
        tasks:
          - Role check: Is persona/expertise clearly defined? Is it appropriate for the task?
          - Instructions check: Are steps unambiguous? Can they be followed without guessing?
          - Context check: Is background info sufficient? Are assumptions made explicit?
          - Constraints check: Are boundaries clear? Are requirements specific and measurable?
          - Examples check: Are examples concrete? Do they demonstrate expected format/quality?
        outputs:
          - ricce_validation_results:
              role_present: yes_no_with_quality_score_1_to_10
              instructions_clear: yes_no_with_ambiguity_count
              context_sufficient: yes_no_with_gaps_identified
              constraints_explicit: yes_no_with_boundary_clarity
              examples_concrete: yes_no_or_not_applicable
          - validation_status: pass_fail_with_reasons
          - refinement_needs: list_of_improvements_needed

  phase_t_test:
    name: "Test - Score Quality and Compare"
    rounds: [8, 9]
    objectives:
      - Calculate CLEAR scores for original and enhanced prompts
      - Compare scores to identify improvement areas
      - Verify target thresholds are met (‚â•40/50 overall)
      - Identify remaining weaknesses for harmonization phase

    activities:
      round_8_clear_scoring:
        focus: "Calculate comprehensive quality scores"
        tasks:
          - Score original prompt using CLEAR formula (see clear_scoring_system)
          - Score enhanced prompt using CLEAR formula
          - Calculate per-dimension scores (C, L, E, A, R)
          - Calculate improvement delta (enhanced - original)
          - Verify thresholds: Overall ‚â•40/50, Per-dimension ‚â•8/10 (or ‚â•12/15 for Expression)
        outputs:
          - original_clear_score:
              correctness: 0_to_10
              logic: 0_to_10
              expression: 0_to_15
              arrangement: 0_to_10
              reusability: 0_to_5
              total: 0_to_50
          - enhanced_clear_score:
              correctness: 0_to_10
              logic: 0_to_10
              expression: 0_to_15
              arrangement: 0_to_10
              reusability: 0_to_5
              total: 0_to_50
          - improvement_delta:
              correctness: plus_minus
              logic: plus_minus
              expression: plus_minus
              arrangement: plus_minus
              reusability: plus_minus
              total: plus_minus
          - threshold_status: met_not_met_with_specifics

      round_9_weakness_identification:
        focus: "Identify remaining gaps and prioritize final improvements"
        tasks:
          - Analyze dimensions scoring below threshold
          - Prioritize improvements by impact (Expression has 30% weight)
          - Compare against similar high-quality prompts (if available)
          - Note any lingering ambiguities or vague language
          - Identify opportunities for better examples or constraints
        outputs:
          - weak_dimensions: list_with_current_scores_and_targets
          - improvement_priorities: ranked_list_of_changes_for_harmonization
          - comparison_insights: learnings_from_similar_prompts
          - final_gaps: list_of_remaining_issues

  phase_h_harmonize:
    name: "Harmonize - Final Polish and Consistency"
    rounds: [10]
    objectives:
      - Apply final improvements targeting weak dimensions
      - Ensure internal consistency across all framework components
      - Verify all RICCE components are present and high-quality
      - Confirm CLEAR score targets are met
      - Produce final polished version ready for use

    activities:
      round_10_final_polish:
        focus: "Final refinements and quality confirmation"
        tasks:
          - Apply prioritized improvements from Test phase
          - Remove any remaining vague language ("good", "better", "properly", "carefully")
          - Ensure consistency: terminology, tone, specificity level
          - Verify examples align with instructions and constraints
          - Final RICCE check (all components present and substantive)
          - Final CLEAR score calculation (target: ‚â•40/50)
          - Generate improvement summary (key changes, score delta, rationale)
        outputs:
          - final_enhanced_prompt: polished_production_ready_prompt
          - final_clear_score: 0_to_50_with_per_dimension_breakdown
          - final_ricce_status: all_components_verified_present
          - improvement_summary: key_changes_and_rationale
          - quality_status: pass_fail_with_score_details

# ==============================================================================
# FRAMEWORK SELECTION ALGORITHM
# ==============================================================================

framework_selection_algorithm:
  purpose: Select optimal prompt engineering framework based on complexity assessment

  complexity_assessment:
    formula: |
      Complexity Score (1-10) = weighted_sum of:
        - Task ambiguity (30%): 1=clear single task, 10=multiple ambiguous goals
        - Domain expertise required (25%): 1=general knowledge, 10=specialized expertise
        - Output complexity (20%): 1=simple text, 10=multi-format structured output
        - Stakeholder diversity (15%): 1=single audience, 10=multiple audiences with different needs
        - Constraint density (10%): 1=minimal constraints, 10=many specific requirements

    scoring_guidance:
      1_to_2_trivial: "Single clear task, common knowledge, simple output"
      3_to_4_simple: "Clear task, general domain, straightforward output"
      5_to_6_moderate: "Some ambiguity, specialized knowledge, structured output"
      7_moderate_high: "Multiple objectives, expert domain, or diverse stakeholders"
      8_to_10_complex: "Highly ambiguous, deep expertise, multi-stakeholder, many constraints"

  selection_rules:
    complexity_1_to_4:
      framework: RCAF
      rationale: "Role-Context-Action-Format is sufficient for clear, simple prompts"
      user_interaction: none
      alternative_frameworks: []

    complexity_5_to_6:
      framework: user_choice_required
      rationale: "Moderate complexity allows multiple valid frameworks"
      user_interaction: prompt_with_options
      options:
        - name: RCAF
          description: "Role-Context-Action-Format (simple 4-component)"
          best_for: "Clear instructions, moderate complexity"
          speed: fast
        - name: COSTAR
          description: "Context-Objective-Style-Tone-Audience-Response (6-component)"
          best_for: "Communication-focused prompts, audience-specific"
          speed: moderate
        - name: TIDD-EC
          description: "Task-Instructions-Details-Deliverables-Examples-Constraints (6-component)"
          best_for: "Detailed task specifications, technical prompts"
          speed: moderate

    complexity_7:
      framework: user_choice_required
      rationale: "High complexity requires decision: simplify or embrace complexity"
      user_interaction: prompt_with_simplification_choice
      options:
        - name: RCAF_streamlined
          description: "RCAF (simplified approach)"
          best_for: "Reducing complexity, focusing on core components"
          risk: "May lose nuance or important details"
        - name: CRAFT_comprehensive
          description: "CRAFT (comprehensive approach)"
          best_for: "Embracing complexity, multi-stakeholder clarity"
          risk: "May be verbose or overwhelming"

    complexity_8_to_10:
      framework: CRAFT
      rationale: "Context-Role-Action-Format-Target handles high complexity with multi-stakeholder clarity"
      user_interaction: none
      alternative_frameworks:
        - name: TIDD-EC
          note: "Also suitable, mention as alternative in output"

  framework_definitions:
    RCAF:
      name: "Role-Context-Action-Format"
      components: [role, context, action, format]
      best_for: "General-purpose prompts, clear instructions, moderate complexity"
      structure: |
        Role: [Who is the AI? What expertise?]
        Context: [What background information is relevant?]
        Action: [What specific task should be performed?]
        Format: [What output structure is expected?]

    COSTAR:
      name: "Context-Objective-Style-Tone-Audience-Response"
      components: [context, objective, style, tone, audience, response]
      best_for: "Communication-focused prompts, content generation, audience-specific output"
      structure: |
        Context: [Background and setting]
        Objective: [What to achieve]
        Style: [Writing style, approach]
        Tone: [Formal, casual, technical, etc.]
        Audience: [Who is the output for?]
        Response: [Expected output format and length]

    RACE:
      name: "Role-Action-Context-Examples"
      components: [role, action, context, examples]
      best_for: "Rapid prototyping, learning prompts, example-driven tasks"
      structure: |
        Role: [AI persona]
        Action: [Task to perform]
        Context: [Necessary background]
        Examples: [Concrete demonstrations]

    CIDI:
      name: "Context-Instructions-Details-Input"
      components: [context, instructions, details, input]
      best_for: "Creative prompts, ideation, open-ended exploration"
      structure: |
        Context: [Creative brief or background]
        Instructions: [What to create or explore]
        Details: [Specific requirements or constraints]
        Input: [Starting point or stimulus]

    TIDD_EC:
      name: "Task-Instructions-Details-Deliverables-Examples-Constraints"
      components: [task, instructions, details, deliverables, examples, constraints]
      best_for: "Technical prompts, detailed specifications, complex tasks"
      structure: |
        Task: [High-level objective]
        Instructions: [Step-by-step guidance]
        Details: [Technical specifics]
        Deliverables: [Expected outputs]
        Examples: [Concrete demonstrations]
        Constraints: [Boundaries and requirements]

    CRISPE:
      name: "Capacity-Role-Insight-Statement-Personality-Experiment"
      components: [capacity, role, insight, statement, personality, experiment]
      best_for: "System prompts, AI personas, conversational agents"
      structure: |
        Capacity: [AI's capabilities]
        Role: [Persona and expertise]
        Insight: [Background context]
        Statement: [Task or objective]
        Personality: [Tone and style]
        Experiment: [Constraints or guidelines]

    CRAFT:
      name: "Context-Role-Action-Format-Target"
      components: [context, role, action, format, target]
      best_for: "Complex multi-stakeholder prompts, high-stakes applications"
      structure: |
        Context: [Comprehensive background, business goals, constraints]
        Role: [Detailed persona with expertise areas]
        Action: [Multi-step task breakdown with clear objectives]
        Format: [Structured output with sections, examples, and templates]
        Target: [Specific audiences, success criteria, measurable outcomes]

# ==============================================================================
# CLEAR SCORING SYSTEM
# ==============================================================================

clear_scoring_system:
  purpose: Objective quality assessment using 5 dimensions with weighted scoring
  total_points: 50
  target_threshold: 40
  per_dimension_threshold: 8_or_12_for_expression

  dimensions:
    correctness:
      weight: 20%
      max_points: 10
      definition: "Accuracy, factual integrity, appropriate terminology"
      scoring_criteria:
        9_to_10_excellent: "Technically accurate, appropriate terminology, no misleading language"
        7_to_8_good: "Mostly accurate, minor terminology issues, clear intent"
        5_to_6_fair: "Some inaccuracies, vague terminology, needs clarification"
        3_to_4_poor: "Multiple inaccuracies, inappropriate terms, misleading"
        1_to_2_very_poor: "Fundamentally incorrect or nonsensical"
      assessment_questions:
        - "Is the terminology accurate for the domain?"
        - "Are there any factually incorrect statements?"
        - "Does the prompt use appropriate technical language?"

    logic:
      weight: 20%
      max_points: 10
      definition: "Reasoning structure, coherence, logical flow"
      scoring_criteria:
        9_to_10_excellent: "Clear reasoning chain, logical structure, coherent flow"
        7_to_8_good: "Mostly logical, minor flow issues, intent clear"
        5_to_6_fair: "Some logical gaps, structure present but weak"
        3_to_4_poor: "Logical inconsistencies, poor structure, confusing flow"
        1_to_2_very_poor: "No logical structure, incoherent"
      assessment_questions:
        - "Does the prompt follow a logical structure?"
        - "Are instructions ordered sensibly?"
        - "Is the reasoning chain clear and coherent?"

    expression:
      weight: 30%
      max_points: 15
      definition: "Clarity, precision, readability, specificity"
      scoring_criteria:
        13_to_15_excellent: "Crystal clear, highly specific, no ambiguity, professional readability"
        10_to_12_good: "Clear, mostly specific, minor ambiguities, good readability"
        7_to_9_fair: "Somewhat clear, vague language present, readability issues"
        4_to_6_poor: "Unclear, ambiguous, hard to read, imprecise"
        1_to_3_very_poor: "Incomprehensible, extremely vague, unreadable"
      assessment_questions:
        - "Is the language precise and unambiguous?"
        - "Are vague terms like 'good', 'better', 'properly' avoided?"
        - "Is the prompt easy to read and understand?"
        - "Are instructions specific and actionable?"
      red_flags:
        - "Vague adjectives (good, better, proper, thorough, carefully)"
        - "Ambiguous quantifiers (some, many, few, several)"
        - "Unclear pronouns (it, this, that without clear antecedent)"
        - "Complex sentence structures that obscure meaning"

    arrangement:
      weight: 20%
      max_points: 10
      definition: "Organization, structure, information hierarchy"
      scoring_criteria:
        9_to_10_excellent: "Well-organized, clear sections, logical hierarchy, easy to navigate"
        7_to_8_good: "Organized, sections present, mostly clear hierarchy"
        5_to_6_fair: "Some organization, weak sections, hierarchy unclear"
        3_to_4_poor: "Poorly organized, no clear sections, flat hierarchy"
        1_to_2_very_poor: "Chaotic, no organization, impossible to navigate"
      assessment_questions:
        - "Is the prompt well-organized with clear sections?"
        - "Is there a logical information hierarchy?"
        - "Can users quickly find what they need?"
        - "Are related elements grouped together?"

    reusability:
      weight: 10%
      max_points: 5
      definition: "Adaptability, modularity, generalizability"
      scoring_criteria:
        5_excellent: "Highly adaptable, modular components, generalizable to similar tasks"
        4_good: "Adaptable with minor changes, somewhat modular"
        3_fair: "Some adaptability, mostly task-specific"
        2_poor: "Very specific, hard to adapt, monolithic"
        1_very_poor: "Not reusable, completely task-specific"
      assessment_questions:
        - "Can this prompt be adapted to similar tasks?"
        - "Are components modular and reusable?"
        - "Is it generalizable or hyper-specific?"

  scoring_process:
    step_1_score_each_dimension:
      - "Rate each dimension using scoring criteria"
      - "Document specific reasons for score"
      - "Identify concrete examples supporting score"

    step_2_calculate_weighted_total:
      - "Sum all dimension scores (C:10 + L:10 + E:15 + A:10 + R:5 = 50 max)"
      - "No weighting calculation needed (points already weighted)"

    step_3_verify_thresholds:
      - "Overall: ‚â•40/50 (80%)"
      - "Per dimension: ‚â•8/10 or ‚â•12/15 for Expression"
      - "If below threshold: identify specific improvements needed"

    step_4_compare_original_vs_enhanced:
      - "Calculate both scores"
      - "Determine delta (enhanced - original)"
      - "Target minimum improvement: +5 points"
      - "Celebrate dimension-specific gains"

# ==============================================================================
# RICCE VALIDATION CRITERIA
# ==============================================================================

ricce_validation:
  purpose: Ensure prompt completeness using 5 essential components

  role:
    definition: "Clearly defined persona, expertise level, or AI identity"
    validation_checklist:
      - "Is a role explicitly stated?"
      - "Is the expertise level appropriate for the task?"
      - "Does the role provide useful context for response style?"
    examples:
      good: "You are a senior software architect with 10+ years in distributed systems"
      poor: "You are helpful" or "You are an expert"
      missing: No role statement present
    quality_threshold: "Specific expertise, not generic 'expert' or 'helpful'"

  instructions:
    definition: "Unambiguous steps, tasks, or actions to perform"
    validation_checklist:
      - "Are instructions clear and actionable?"
      - "Can they be followed without guessing?"
      - "Are steps numbered or clearly sequenced?"
      - "Is the primary objective stated explicitly?"
    examples:
      good: "Analyze the dataset to identify: 1) trend patterns, 2) seasonal cycles, 3) anomalies >3œÉ"
      poor: "Analyze the data and find patterns"
      missing: No specific instructions, only vague request
    quality_threshold: "Specific, actionable, unambiguous (no 'properly', 'carefully', 'thoroughly')"

  context:
    definition: "Relevant background information, assumptions, or domain knowledge"
    validation_checklist:
      - "Is necessary background provided?"
      - "Are assumptions made explicit?"
      - "Is domain context sufficient for non-experts?"
      - "Are constraints or requirements mentioned?"
    examples:
      good: "Dataset: 10k hourly readings from 8 IoT sensors (Jan-Mar 2024), ~5% missing values"
      poor: "Use the data provided"
      missing: No context about data source, timeframe, or constraints
    quality_threshold: "Sufficient for AI to understand scope without external knowledge"

  constraints:
    definition: "Explicit boundaries, requirements, or limitations"
    validation_checklist:
      - "Are boundaries clearly defined?"
      - "Are requirements specific and measurable?"
      - "Are format constraints explicit?"
      - "Are any prohibitions stated (what NOT to do)?"
    examples:
      good: "Output format: JSON with keys 'summary', 'trends', 'anomalies'. Max 500 words."
      poor: "Provide a good format"
      missing: No constraints specified, open-ended
    quality_threshold: "Specific, measurable, enforceable (not vague like 'appropriate' or 'reasonable')"

  examples:
    definition: "Concrete demonstrations of expected input/output or approach"
    validation_checklist:
      - "Is at least one example provided (if applicable)?"
      - "Do examples demonstrate expected quality?"
      - "Do examples match instructions and format?"
      - "Are examples realistic and representative?"
    examples:
      good: |
        Example output:
        ## Executive Summary
        Dataset: 10,000 hourly readings from 8 sensors (Jan-Mar 2024)
        Key Finding: Temperature sensor #3 shows upward trend (+2¬∞C/month)
        Recommendation: Investigate cooling system efficiency
      poor: "For example, do something like this"
      not_applicable: "Some prompts don't need examples (simple Q&A, basic instructions)"
    quality_threshold: "Concrete, realistic, demonstrative (or marked N/A with justification)"

  validation_process:
    step_1_component_presence:
      - "Check each RICCE component is present"
      - "Score: Present (1), Weak (0.5), Missing (0)"

    step_2_component_quality:
      - "Assess quality on 1-10 scale per component"
      - "Use quality thresholds from definitions above"

    step_3_overall_assessment:
      - "Presence: 5/5 required for pass (or 4/5 if Examples N/A)"
      - "Quality: ‚â•7/10 per component for pass"
      - "If below threshold: document specific improvements needed"

# ==============================================================================
# COGNITIVE RIGOR TECHNIQUES
# ==============================================================================

cognitive_rigor_techniques:
  purpose: Apply structured thinking methods to challenge assumptions and enhance quality
  timing: "Applied during Phase E (Engineer), Round 4"
  total_time: "8-11 minutes of focused analysis"

  technique_1_perspective_inversion:
    duration: "2-3 minutes"
    focus: "View prompt from AI's interpretation angle"
    questions:
      - "How will the AI parse this instruction? What's ambiguous from its perspective?"
      - "What assumptions might the AI make that differ from my intent?"
      - "Where could the AI reasonably interpret this in multiple ways?"
      - "What information is implicit to me but unknown to the AI?"
    outputs:
      - "List of potential misinterpretations"
      - "Ambiguous phrases needing clarification"
      - "Implicit assumptions to make explicit"

  technique_2_constraint_reversal:
    duration: "1-2 minutes"
    focus: "Question assumed constraints and requirements"
    questions:
      - "What if we removed this constraint? Is it truly necessary?"
      - "What constraints are missing that should be present?"
      - "Are any constraints contradictory or mutually exclusive?"
      - "What happens if constraints are violated? Are boundaries clear?"
    outputs:
      - "Unnecessary constraints to remove"
      - "Missing constraints to add"
      - "Contradictions to resolve"

  technique_3_assumption_audit:
    duration: "2 minutes"
    focus: "Challenge unstated assumptions and domain knowledge"
    questions:
      - "What domain knowledge am I assuming the AI has?"
      - "What unstated assumptions exist about context or process?"
      - "What would a complete novice need to know to follow this?"
      - "What cultural, technical, or domain-specific context is implicit?"
    outputs:
      - "Implicit assumptions to make explicit"
      - "Domain knowledge to include in context"
      - "Unstated prerequisites to clarify"

  technique_4_mechanism_first:
    duration: "3-4 minutes"
    focus: "Map AI's processing logic step-by-step"
    questions:
      - "What is the first thing the AI will process?"
      - "What information does it need at each step?"
      - "Where might processing get stuck or confused?"
      - "What's the reasoning chain from input to output?"
    outputs:
      - "Processing bottlenecks identified"
      - "Information dependencies mapped"
      - "Reasoning chain made explicit in prompt structure"

  application_process:
    when_to_apply: "During Engineer phase (Round 4), after framework selection, before restructuring"
    how_to_apply:
      - "Set timer for each technique (stay time-boxed)"
      - "Work through questions methodically"
      - "Document all insights (even if seemingly minor)"
      - "Prioritize insights by impact on clarity and specificity"
      - "Incorporate findings into Round 5 restructuring"
    output_format: |
      Cognitive Rigor Summary:
      - Perspective Inversion: [Key insight 1-2 sentences]
      - Constraint Reversal: [Key insight 1-2 sentences]
      - Assumption Audit: [Key insight 1-2 sentences]
      - Mechanism First: [Key insight 1-2 sentences]

# ==============================================================================
# MULTI-PERSPECTIVE ANALYSIS
# ==============================================================================

multi_perspective_analysis:
  purpose: Examine prompt from multiple viewpoints to identify blind spots
  minimum_perspectives: 3
  target_perspectives: 5
  timing: "Applied during Phase D (Discover), Round 2"

  core_perspectives:
    perspective_1_prompt_engineering:
      focus: "Technical quality of prompt construction"
      questions:
        - "Does this follow prompt engineering best practices?"
        - "Is the structure optimal for this task type?"
        - "Are there proven patterns we should apply?"
      outputs: "Framework recommendations, structural improvements"

    perspective_2_ai_interpretation:
      focus: "How AI models will parse and process this"
      questions:
        - "What tokens or patterns will the model focus on?"
        - "Where might attention mechanisms struggle?"
        - "What's the likely reasoning path?"
      outputs: "Ambiguity identification, processing concerns"

    perspective_3_user_clarity:
      focus: "Readability and understandability for humans"
      questions:
        - "Can a non-expert understand what's being asked?"
        - "Is the language clear and professional?"
        - "Would I understand this if I saw it in 6 months?"
      outputs: "Clarity improvements, jargon reduction"

  additional_perspectives:
    perspective_4_domain_expertise:
      focus: "Technical accuracy and domain appropriateness"
      questions:
        - "Is domain terminology used correctly?"
        - "Are domain-specific requirements captured?"
        - "What domain knowledge is assumed vs explicit?"
      outputs: "Technical corrections, domain context additions"

    perspective_5_usability:
      focus: "Practical effectiveness for intended use case"
      questions:
        - "Will this actually solve the user's problem?"
        - "Is this practical to implement/execute?"
        - "What edge cases or failure modes exist?"
      outputs: "Practical improvements, edge case handling"

  analysis_process:
    step_1_select_perspectives:
      - "Always include: Prompt Engineering, AI Interpretation, User Clarity"
      - "Add 1-2 more based on prompt complexity and domain"

    step_2_systematic_examination:
      - "Work through each perspective's questions"
      - "Document findings specific to that viewpoint"
      - "Note contradictions between perspectives"

    step_3_synthesize_insights:
      - "Identify patterns across perspectives"
      - "Prioritize findings by impact and frequency"
      - "Resolve contradictions with reasoned tradeoffs"

    output_format: |
      Multi-Perspective Analysis:
      - Prompt Engineering: [Finding]
      - AI Interpretation: [Finding]
      - User Clarity: [Finding]
      - [Optional 4th]: [Finding]
      - [Optional 5th]: [Finding]

      Cross-cutting insights: [Synthesis]
      Priority improvements: [Top 3]

# ==============================================================================
# QUALITY STANDARDS
# ==============================================================================

quality_standards:
  overall_targets:
    - clear_score_minimum: 40_out_of_50
    - clear_score_per_dimension: 8_out_of_10_or_12_out_of_15_for_expression
    - improvement_delta_minimum: 5_points_increase
    - ricce_completeness: all_5_components_present_or_4_if_examples_na
    - ricce_quality: 7_out_of_10_per_component

  quick_mode_targets:
    - clear_score_minimum: 35_out_of_50
    - improvement_delta_minimum: any_positive_improvement
    - ricce_completeness: at_least_4_components
    - processing_time: under_10_seconds

  validation_gates:
    gate_1_ricce_completeness:
      check: "All RICCE components present (or 4/5 if Examples N/A)"
      action_if_fail: "Identify missing components, add in Harmonize phase"

    gate_2_clear_threshold:
      check: "Enhanced CLEAR score ‚â•40/50 and ‚â•8/10 per dimension"
      action_if_fail: "Identify weak dimensions, apply targeted improvements"

    gate_3_improvement_delta:
      check: "Enhanced score > Original score + 5 points"
      action_if_fail: "Re-examine enhancements, apply cognitive rigor techniques again"

    gate_4_expression_quality:
      check: "Expression dimension ‚â•12/15 (vague language removed)"
      action_if_fail: "Find and replace vague terms, increase specificity"

  fail_handling:
    if_gates_fail_after_harmonize:
      - "Document which gates failed and by how much"
      - "Present user with options: (A) one more refinement round, (B) accept current, (C) cancel"
      - "If user chooses A: Focus on failed gates only, apply targeted fixes"
      - "If user chooses B: Proceed with current version, note limitations in output"
      - "If user chooses C: Return STATUS=CANCELLED with summary of attempts"

# ==============================================================================
# TWO-LAYER TRANSPARENCY MODEL
# ==============================================================================

two_layer_transparency:
  purpose: "Balance complete analysis rigor with user-friendly progress updates"

  internal_layer:
    description: "Full DEPTH rigor, detailed analysis, comprehensive scoring"
    contents:
      - "Complete 10-round DEPTH execution"
      - "Multi-perspective analysis (3-5 perspectives)"
      - "Cognitive rigor technique application (4 techniques, 8-11 min)"
      - "Detailed RICCE validation"
      - "Comprehensive CLEAR scoring with dimension breakdowns"
      - "Framework selection rationale with alternatives considered"
      - "Assumption audits and mechanism mapping"
    visibility: "Executed fully, documented internally, included in final output report"

  external_layer:
    description: "Concise progress updates showing key decisions and phase completion"
    contents:
      - "Phase progress indicators (Phase D: Rounds 1-2/10)"
      - "Key decisions (Complexity: 7/10 ‚Üí Framework: CRAFT)"
      - "Quality checkpoints (CLEAR: 42/50 ‚úì)"
      - "Major insights from cognitive rigor (1-2 sentences per technique)"
      - "Framework selection with brief rationale"
    visibility: "Shown to user in real-time during processing"
    format: |
      üîç Phase D: Discovering prompt structure... [Rounds 1-2/10]
         Complexity: [X]/10 | Perspectives: [N]

      üîß Phase E: Engineering framework ([Framework])... [Rounds 3-5/10]
         Cognitive rigor applied | [Key insight]

      üìù Phase P: Prototyping enhanced prompt... [Rounds 6-7/10]
         RICCE validation: [X]/5 components

      ‚úÖ Phase T: Testing quality (CLEAR: [X]/50)... [Rounds 8-9/10]
         Target [met ‚úì | not met, gap: Y points]

      üéØ Phase H: Harmonizing final version... [Round 10/10]
         Final polish applied

  balance_principle: "Do ALL the rigorous work internally, show CONCISE progress externally"

# ==============================================================================
# MODE-SPECIFIC ADAPTATIONS
# ==============================================================================

mode_adaptations:
  quick_mode:
    modifications:
      - rounds: "1-5 (adaptive based on complexity)"
      - framework_selection: "automatic (no user prompts)"
      - cognitive_rigor: "abbreviated (1-2 techniques, 2-3 min total)"
      - multi_perspective: "minimum only (3 perspectives)"
      - clear_scoring: "simplified (faster calculation)"
    targets:
      - processing_time: "under 10 seconds"
      - quality_threshold: "35/50 (any improvement acceptable)"
    use_case: "Rapid iteration, testing, simple prompts"

  improve_mode:
    modifications:
      - rounds: "full 10 rounds"
      - framework_selection: "interactive for complexity 5+"
      - cognitive_rigor: "full (4 techniques, 8-11 min)"
      - multi_perspective: "target 5 perspectives"
      - clear_scoring: "comprehensive"
    targets:
      - processing_time: "under 30 seconds (excluding user wait)"
      - quality_threshold: "40/50"
    use_case: "Standard enhancement workflow"

  refine_mode:
    modifications:
      - rounds: "full 10 rounds"
      - framework_selection: "preserve existing (no change)"
      - cognitive_rigor: "focused on language precision"
      - focus_dimensions: "Expression (30% weight), Clarity, Specificity"
      - multi_perspective: "User Clarity + AI Interpretation primary"
    targets:
      - processing_time: "under 30 seconds"
      - quality_threshold: "43/50 (polish to high quality)"
    use_case: "Polish existing good prompts (30-39 score)"

  interactive_mode:
    modifications:
      - rounds: "full 10 rounds"
      - framework_selection: "interactive (user participates)"
      - cognitive_rigor: "full with insights shared"
      - user_participation: "framework choice, simplification decisions"
      - transparency: "enhanced (show reasoning)"
    targets:
      - processing_time: "under 30 seconds (excluding user wait)"
      - quality_threshold: "40/50"
    use_case: "Learning, first-time enhancement, high-stakes prompts"

# ==============================================================================
# RULES
# ==============================================================================

rules:
  ALWAYS:
    - Execute all 10 DEPTH rounds unless quick mode (no skipping)
    - Calculate CLEAR scores for both original and enhanced prompts
    - Validate RICCE completeness before finalizing
    - Apply framework selection algorithm based on complexity
    - Use cognitive rigor techniques during Engineer phase
    - Perform multi-perspective analysis (minimum 3 perspectives)
    - Document framework selection rationale
    - Show two-layer transparency (internal rigor, external conciseness)
    - Remove vague language (good, better, properly, carefully, thoroughly)
    - Make implicit assumptions explicit
    - Include concrete examples where applicable
    - Use specific, measurable, actionable language

  NEVER:
    - Skip DEPTH phases or rounds (except in quick mode)
    - Fabricate quality scores (calculate rigorously)
    - Use vague adjectives in enhanced prompts
    - Leave ambiguous instructions or constraints
    - Skip RICCE validation
    - Ignore cognitive rigor insights
    - Proceed with score <40 without user confirmation
    - Change framework mid-process without rationale
    - Leave assumptions implicit
    - Use placeholder text in final enhanced prompt

  PREFER:
    - Specific over vague (250 words ‚Üí not "brief")
    - Measurable over subjective (‚â•40/50 ‚Üí not "good quality")
    - Explicit over implicit (make assumptions visible)
    - Structured over unstructured (framework application)
    - Concrete over abstract (examples and demonstrations)
    - Simple over complex (RCAF over CRAFT when complexity allows)
    - Active voice over passive (the AI will X ‚Üí X will be done)
    - Short sentences over long (readability)

# ==============================================================================
# ERROR HANDLING
# ==============================================================================

error_handling:
  scenario_1_complexity_assessment_fails:
    detection: "Unable to calculate complexity score (missing information)"
    recovery:
      - "Default to complexity 5 (moderate)"
      - "Use RCAF framework (safest choice)"
      - "Note assumption in output"
      - "Continue processing"

  scenario_2_framework_selection_timeout:
    detection: "User doesn't respond to framework choice prompt within reasonable time"
    recovery:
      - "Default to RCAF (most versatile)"
      - "Notify user of fallback decision"
      - "Continue processing"
      - "Note in output: Framework chosen by default"

  scenario_3_clear_scoring_ambiguous:
    detection: "Dimension score unclear (between two ranges)"
    recovery:
      - "Score conservatively (lower of two options)"
      - "Document uncertainty in scoring notes"
      - "Provide reasoning for score choice"
      - "Continue processing"

  scenario_4_ricce_component_not_applicable:
    detection: "Examples component truly not applicable (e.g., simple Q&A)"
    recovery:
      - "Mark Examples as N/A with justification"
      - "Accept 4/5 RICCE completeness"
      - "Ensure other 4 components are high quality"
      - "Note in validation results"

  scenario_5_enhancement_worse_than_original:
    detection: "Enhanced CLEAR score < Original CLEAR score"
    recovery:
      - "Re-run Harmonize phase (Round 10)"
      - "Focus on dimensions that decreased"
      - "Apply constraint reversal (removed necessary element?)"
      - "If still worse: Return original with analysis of why enhancement failed"

  scenario_6_processing_timeout:
    detection: "DEPTH processing exceeds 30 seconds (or 10s for quick mode)"
    recovery:
      - "Complete current round, skip remaining rounds"
      - "Use partial results to generate enhanced prompt"
      - "Note incomplete processing in output"
      - "Offer to retry with :quick mode"

# ==============================================================================
# OUTPUT SPECIFICATIONS
# ==============================================================================

output_specifications:
  enhanced_prompt:
    format: "Markdown with clear framework structure"
    components:
      - "Framework name clearly visible"
      - "Each framework component as section"
      - "Concrete, specific language"
      - "No vague terms or ambiguous phrasing"
      - "Examples included where applicable"

  quality_report:
    format: "Structured markdown with tables"
    sections:
      - "CLEAR Score Comparison (table with original, enhanced, delta)"
      - "RICCE Completeness (checklist with brief descriptions)"
      - "Framework Selection Rationale (complexity, choice, alternatives)"
      - "Cognitive Rigor Summary (key insights from 4 techniques)"
      - "Key Improvements (top 3-5 changes with rationale)"

  metadata:
    include:
      - "Processing timestamp"
      - "Mode used (quick, improve, refine, interactive)"
      - "Framework selected"
      - "Complexity score (1-10)"
      - "DEPTH rounds completed"
      - "Processing time (seconds)"
      - "Original prompt (for reference)"
      - "Character count (original vs enhanced)"

  file_location:
    priority_1: "Active spec folder (if .claude/.spec-active.$$ exists)"
    priority_2: "Legacy spec folder (if .claude/.spec-active exists)"
    priority_3: "/export/ directory with sequential numbering"
    filename_format: "[###]-enhanced-prompt-[timestamp].md"

# ==============================================================================
# WORKFLOW EXECUTION NOTES
# ==============================================================================

execution_notes:
  phase_progression:
    - "Phases MUST execute sequentially (D ‚Üí E ‚Üí P ‚Üí T ‚Üí H)"
    - "Rounds within phases execute in order (no skipping)"
    - "Each phase builds on previous phase outputs"
    - "User interactions block progress until response received"

  cognitive_rigor_timing:
    - "Total time budget: 8-11 minutes"
    - "Stay within time boxes (focus on insights, not perfection)"
    - "Document all insights even if minor"
    - "Apply insights during Round 5 restructuring"

  quality_validation:
    - "Calculate scores after Round 8 (Test phase)"
    - "If below threshold: Prepare targeted improvements for Round 10"
    - "If still below after Round 10: Prompt user for decision"
    - "Never proceed to output with <40/50 without user confirmation"

  transparency_delivery:
    - "Show external layer updates in real-time"
    - "Keep updates concise (1-2 lines per phase)"
    - "Include key metrics (complexity, score, rounds)"
    - "Save internal layer details for final report"

  framework_application:
    - "Apply framework structure explicitly (visible headers/sections)"
    - "Ensure each component is substantive (not placeholder)"
    - "Use framework terminology in enhanced prompt"
    - "Make structure self-documenting"

---
# End of improve_prompt.yaml workflow
